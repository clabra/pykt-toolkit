{
  "_comment": "GainAKT3Exp parameter defaults - Updated 2025-11-17: V2 tuning for skill differentiation",
  "_bug_fix_adjustment": "After fixing scalar gains bug (per-skill gains implemented), need stronger IM loss and variance loss to encourage skill differentiation.",
  "_change_log": "2025-11-17 V2: Increased IM loss 30%\u219250% (bce=0.5), added variance_loss_weight=0.1, epochs=20, gains_projection lr boost via layer-wise optimizer.",
  "defaults": {
    "seed": 42,
    "epochs": 20,
    "batch_size": 64,
    "learning_rate": 0.000174,
    "weight_decay": 1.7571e-05,
    "optimizer": "Adam",
    "gradient_clip": 1.0,
    "patience": 10,
    "monitor_freq": 50,
    "use_amp": false,
    "use_wandb": false,
    "auto_shifted_eval": true,
    "max_correlation_students": 3800,
    "seq_len": 200,
    "d_model": 256,
    "n_heads": 4,
    "num_encoder_blocks": 4,
    "d_ff": 512,
    "dropout": 0.2,
    "emb_type": "qid",
    "use_mastery_head": true,
    "use_gain_head": true,
    "use_skill_difficulty": false,
    "use_student_speed": false,
    "bce_loss_weight": 0.5,
    "beta_skill_init": 2.0,
    "m_sat_init": 0.8,
    "gamma_student_init": 1.0,
    "sigmoid_offset": 2.0,
    "mastery_threshold_init": 0.85,
    "threshold_temperature": 1.0,
    "num_students": 3055,
    "dataset": "assist2015",
    "fold": 0,
    "train_script": "examples/train_gainakt3exp.py",
    "eval_script": "examples/eval_gainakt3exp.py",
    "model": "gainakt3exp",
    "variance_loss_weight": 0.1
  },
  "deprecated": {
    "_comment": "Parameters deprecated as of 2025-11-16. Commented out in argparse. DO NOT use in new experiments.",
    "_total_count": 27,
    "_date": "2025-11-16",
    "_reason": "Architecture simplification to dual-encoder with BCE + Incremental Mastery Loss only",
    "_num_students_comment": "DEPRECATED: This parameter is automatically overridden by training script to match the actual training set size. Training script extracts num_students from dataset, making code generalizable to any dataset. The value here (3055) was historically for assist2015 fold 0 validation split, but actual training uses all other folds (12220 students).",
    "constraint_losses": {
      "non_negative_loss_weight": {
        "value": 0.0,
        "reason": "All constraint losses commented out"
      },
      "monotonicity_loss_weight": {
        "value": 0.0,
        "reason": "All constraint losses commented out"
      },
      "mastery_performance_loss_weight": {
        "value": 0.0,
        "reason": "Replaced by Incremental Mastery Loss"
      },
      "gain_performance_loss_weight": {
        "value": 0.0,
        "reason": "Gains not exposed (use_gain_head=false)"
      },
      "sparsity_loss_weight": {
        "value": 0.0,
        "reason": "Sparsity constraint removed"
      },
      "consistency_loss_weight": {
        "value": 0.0,
        "reason": "Consistency constraint removed"
      }
    },
    "semantic_alignment": {
      "enable_alignment_loss": {
        "value": false,
        "reason": "Semantic alignment module disabled"
      },
      "alignment_weight": {
        "value": 0.0,
        "reason": "Feature disabled (was optimal: 0.15)",
        "previous_optimal": 0.15
      },
      "alignment_warmup_epochs": {
        "value": 8,
        "reason": "Only used when enable_alignment_loss=true"
      },
      "adaptive_alignment": {
        "value": false,
        "reason": "Only used when enable_alignment_loss=true"
      },
      "alignment_min_correlation": {
        "value": 0.05,
        "reason": "Only used when enable_alignment_loss=true"
      },
      "alignment_share_cap": {
        "value": 0.08,
        "reason": "Only used when enable_alignment_loss=true"
      },
      "alignment_share_decay_factor": {
        "value": 0.7,
        "reason": "Only used when enable_alignment_loss=true"
      },
      "use_residual_alignment": {
        "value": false,
        "reason": "Only used when enable_alignment_loss=true"
      },
      "alignment_residual_window": {
        "value": 5,
        "reason": "Only used with use_residual_alignment=true"
      }
    },
    "global_alignment": {
      "enable_global_alignment_pass": {
        "value": false,
        "reason": "Global alignment module disabled"
      },
      "alignment_global_students": {
        "value": 600,
        "reason": "Only used when enable_global_alignment_pass=true"
      }
    },
    "semantic_refinement": {
      "enable_retention_loss": {
        "value": false,
        "reason": "Retention loss module disabled"
      },
      "retention_delta": {
        "value": 0.005,
        "reason": "Only used when enable_retention_loss=true"
      },
      "retention_weight": {
        "value": 0.0,
        "reason": "Only used when enable_retention_loss=true"
      },
      "enable_lag_gain_loss": {
        "value": false,
        "reason": "Lag gain loss module disabled"
      },
      "lag_gain_weight": {
        "value": 0.0,
        "reason": "Only used when enable_lag_gain_loss=true"
      },
      "lag_max_lag": {
        "value": 3,
        "reason": "Only used when enable_lag_gain_loss=true"
      },
      "lag_l1_weight": {
        "value": 0.5,
        "reason": "Only used when enable_lag_gain_loss=true"
      },
      "lag_l2_weight": {
        "value": 0.3,
        "reason": "Only used when enable_lag_gain_loss=true"
      },
      "lag_l3_weight": {
        "value": 0.2,
        "reason": "Only used when enable_lag_gain_loss=true"
      },
      "consistency_rebalance_epoch": {
        "value": 8,
        "reason": "Consistency rebalancing removed"
      },
      "consistency_rebalance_threshold": {
        "value": 0.1,
        "reason": "Consistency rebalancing removed"
      },
      "consistency_rebalance_new_weight": {
        "value": 0.2,
        "reason": "Consistency rebalancing removed"
      },
      "variance_floor": {
        "value": 0.0001,
        "reason": "Variance floor dynamics removed"
      },
      "variance_floor_patience": {
        "value": 3,
        "reason": "Variance floor dynamics removed"
      },
      "variance_floor_reduce_factor": {
        "value": 0.5,
        "reason": "Variance floor dynamics removed"
      }
    },
    "warmup_scheduling": {
      "warmup_constraint_epochs": {
        "value": 8,
        "reason": "Not used (constraint losses disabled)"
      },
      "max_semantic_students": {
        "value": 50,
        "reason": "Not used (semantic modules disabled)"
      },
      "enable_cosine_perf_schedule": {
        "value": false,
        "reason": "Not used (performance alignment disabled)"
      }
    },
    "deprecated_architecture": {
      "intrinsic_gain_attention": {
        "value": false,
        "reason": "Deprecated mode, argparse commented out"
      }
    }
  },
  "types": {
    "launcher": [
      "train_script",
      "eval_script"
    ],
    "data": [
      "dataset",
      "fold"
    ],
    "runtime": [
      "seed",
      "epochs",
      "batch_size",
      "learning_rate",
      "weight_decay",
      "optimizer",
      "gradient_clip",
      "patience",
      "monitor_freq",
      "use_amp",
      "use_wandb",
      "auto_shifted_eval",
      "max_correlation_students"
    ],
    "model_config": [
      "seq_len",
      "d_model",
      "n_heads",
      "num_encoder_blocks",
      "d_ff",
      "dropout",
      "emb_type"
    ],
    "interpretability": [
      "use_mastery_head",
      "use_gain_head",
      "use_skill_difficulty",
      "use_student_speed",
      "bce_loss_weight",
      "mastery_threshold_init",
      "threshold_temperature",
      "num_students"
    ],
    "learningcurves": [
      "beta_skill_init",
      "m_sat_init",
      "gamma_student_init",
      "sigmoid_offset"
    ],
    "legacy": [
      "model",
      "enhanced_constraints"
    ]
  },
  "fixed": {
    "model": "gainakt3exp",
    "train_script": "examples/train_gainakt3exp.py",
    "eval_script": "examples/eval_gainakt3exp.py"
  },
  "md5": "34a64bd6f2e63d7b65ca203554cdb5ae",
  "_md5_computed": "2025-11-16"
}