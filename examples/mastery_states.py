#!/usr/bin/env python3
"""
Mastery States Analyzer for Knowledge Tracing Models

This script calculates mastery states for each skill practiced in each question
at each time step. Works with both single-skill and multi-skill questions.

Usage:
    # Analyze test set (default: 20 students)
    python examples/mastery_states.py --run_dir <experiment_dir> --split test
    
    # Analyze with custom number of students
    python examples/mastery_states.py --run_dir <experiment_dir> --split test --num_students 50
    
    # Analyze training set
    python examples/mastery_states.py --run_dir <experiment_dir> --split train --num_students 20
    
    # Run command from config.json (automatically generated during training)
    # The command is stored in config.json under commands.mastery_states

The script generates:
    - mastery_states_{split}.csv: Complete mastery state trajectory for all students
      Columns: student_id, time_step, question_id, skill_id, response, mastery_state
      
    - mastery_states_summary_{split}.json: Aggregate statistics about mastery progression
      Contains: per-skill statistics (mean, std, count, range), progression samples
      
Output Format:
    mastery_states_{split}.csv:
        - student_id: Unique student identifier
        - time_step: Sequential position in student's learning trajectory (0-indexed)
        - question_id: Question attempted at this time step
        - skill_id: Skill/concept being assessed (for single-skill: skill_id == question_id)
        - response: Student's response (1=correct, 0=incorrect)
        - mastery_state: Model's estimated mastery level for this skill at this time step
          (continuous value from positivity constraint, monotonically increasing over time)
    
    mastery_states_summary_{split}.json:
        - total_observations: Total number of (student, time_step, skill) tuples
        - num_concepts: Total number of skills/concepts in the dataset
        - skills_observed: Number of skills that appeared in the data
        - skill_statistics: Per-skill aggregate metrics and temporal progression samples

Notes:
    - For single-skill datasets (like assist2015): one skill per question
    - For multi-skill datasets: multiple rows per question, one per skill
    - Mastery states are extracted from the model's skill_vector (KC vector)
    - Monotonicity constraint ensures mastery_state[t+1] >= mastery_state[t]
"""

import os
import sys
import torch
import numpy as np
import argparse
import json
import csv
from collections import defaultdict
from datetime import datetime

# Add project root to path
sys.path.insert(0, '/workspaces/pykt-toolkit')

from pykt.datasets import init_dataset4train
from pykt.datasets.data_loader import KTDataset
from torch.utils.data import DataLoader
from pykt.models.gainakt4 import GainAKT4


def load_model_and_config(run_dir, ckpt_name):
    """Load trained model and configuration."""
    # Load config
    config_path = os.path.join(run_dir, 'config.json')
    with open(config_path, 'r') as f:
        full_config = json.load(f)
    
    #config = full_config['defaults']
    # Merge defaults with overrides (overrides take precedence)
    config = full_config['defaults'].copy()
    config.update(full_config.get('overrides', {}))
    
    # Setup data config
    # Load data config from configs/data_config.json
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_path = os.path.join(project_root, 'configs', 'data_config.json')
    with open(config_path, 'r') as f:
        data_config = json.load(f)
    
    # Convert relative paths to absolute paths
    for dataset_name in data_config:
        if 'dpath' in data_config[dataset_name] and data_config[dataset_name]['dpath'].startswith('../'):
            data_config[dataset_name]['dpath'] = os.path.join(project_root, data_config[dataset_name]['dpath'][3:])
    
    if config['dataset'] not in data_config:
        raise ValueError(f"Dataset '{config['dataset']}' not found in data_config.json. Available: {list(data_config.keys())}")
    
    num_c = data_config[config['dataset']]['num_c']
    
    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = GainAKT4(
        num_c=num_c,
        seq_len=config['seq_len'],
        d_model=config['d_model'],
        n_heads=config['n_heads'],
        num_encoder_blocks=config['num_encoder_blocks'],
        d_ff=config['d_ff'],
        dropout=config['dropout'],
        emb_type=config['emb_type'],
        lambda_bce=config['lambda_bce']
    ).to(device)
    
    # Multi-GPU support: wrap model with DataParallel if multiple GPUs available
    if device.type == 'cuda' and torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    
    # Load checkpoint
    checkpoint_path = os.path.join(run_dir, ckpt_name)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_to_load = model.module if isinstance(model, torch.nn.DataParallel) else model
    model_to_load.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    return model, config, data_config, device, num_c


def extract_mastery_states(model, data_loader, device, num_concepts, max_students=None):
    """
    Extract mastery states for each skill at each time step.
    
    Args:
        max_students: Maximum number of students to process (None for all)
    
    Returns:
        mastery_data: List of dicts with student_id, time_step, question_id, skills, responses, mastery_states
    """
    model.eval()
    mastery_data = []
    students_processed = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(data_loader):
            # Check if we've reached the student limit
            if max_students is not None and students_processed >= max_students:
                break
            
            questions = batch['cseqs'].to(device)  # [B, L]
            responses = batch['rseqs'].to(device)  # [B, L]
            questions_shifted = batch['shft_cseqs'].to(device)  # [B, L]
            mask = batch['masks'].to(device)  # [B, L]
            
            # Get student IDs if available
            if 'uids' in batch:
                student_ids = batch['uids'].cpu().numpy()
            else:
                # Generate sequential IDs if not available
                student_ids = np.arange(batch_idx * questions.shape[0], 
                                       (batch_idx + 1) * questions.shape[0])
            
            # Forward pass to get skill vectors (mastery states)
            outputs = model(q=questions, r=responses, qry=questions_shifted)
            
            # Check if mastery head is active (skill_vector will be None when Œª=1.0)
            if outputs['skill_vector'] is None:
                print("\n" + "="*80)
                print("‚ö†Ô∏è  MASTERY STATES NOT AVAILABLE")
                print("="*80)
                print("The model was trained with Œª_bce=1.0 (pure BCE mode).")
                print("Mastery head (Head 2) was not computed, so skill vectors are unavailable.")
                print("Mastery states analysis requires Œª_bce < 1.0 to activate Head 2.")
                print("="*80)
                return []
            
            # skill_vector contains mastery state for each concept at each time step
            # Shape: [B, L, num_concepts]
            skill_vector = outputs['skill_vector'].cpu().numpy()
            
            # Convert to numpy for processing
            questions_np = questions.cpu().numpy()
            responses_np = responses.cpu().numpy()
            mask_np = mask.cpu().numpy()
            
            # Process each student in the batch
            batch_size, seq_len = questions_np.shape
            
            for student_idx in range(batch_size):
                # Check student limit
                if max_students is not None and students_processed >= max_students:
                    break
                
                student_id = student_ids[student_idx]
                students_processed += 1
                
                # Process each time step
                for time_step in range(seq_len):
                    # Check if this is a valid step (not padding)
                    if mask_np[student_idx, time_step] == 0:
                        continue
                    
                    question_id = int(questions_np[student_idx, time_step])
                    response = int(responses_np[student_idx, time_step])
                    
                    # Get mastery states for all skills at this time step
                    # Note: In single-skill datasets like assist2015, typically only one skill per question
                    # But this approach works for multi-skill questions too
                    mastery_vector = skill_vector[student_idx, time_step, :]
                    
                    # Find which skills are involved in this question
                    # For single-skill: question_id corresponds to skill_id (concept_id)
                    # For multi-skill: would need additional mapping data
                    skills_involved = [question_id]  # Simplified for single-skill case
                    
                    # Extract mastery values for involved skills
                    mastery_values = {
                        skill_id: float(mastery_vector[skill_id]) 
                        for skill_id in skills_involved if skill_id < num_concepts
                    }
                    
                    # Store the data
                    mastery_data.append({
                        'student_id': int(student_id),
                        'time_step': int(time_step),
                        'question_id': question_id,
                        'skills': skills_involved,
                        'response': response,
                        'mastery_states': mastery_values,
                        'batch_idx': batch_idx
                    })
    
    return mastery_data


def compute_mastery_statistics(mastery_data, num_concepts):
    """Compute aggregate statistics about mastery progression."""
    
    # Organize by skill
    skill_mastery = defaultdict(list)
    skill_progression = defaultdict(lambda: defaultdict(list))
    
    for entry in mastery_data:
        for skill_id, mastery_value in entry['mastery_states'].items():
            skill_mastery[skill_id].append(mastery_value)
            skill_progression[skill_id][entry['time_step']].append(mastery_value)
    
    # Compute statistics
    statistics = {
        'total_observations': len(mastery_data),
        'num_concepts': num_concepts,
        'skills_observed': len(skill_mastery),
        'skill_statistics': {}
    }
    
    for skill_id in skill_mastery:
        values = skill_mastery[skill_id]
        statistics['skill_statistics'][int(skill_id)] = {
            'count': len(values),
            'mean': float(np.mean(values)),
            'std': float(np.std(values)),
            'min': float(np.min(values)),
            'max': float(np.max(values)),
            'median': float(np.median(values))
        }
        
        # Add progression over time if available
        if skill_progression[skill_id]:
            time_steps = sorted(skill_progression[skill_id].keys())
            progression = [
                {
                    'time_step': int(t),
                    'mean_mastery': float(np.mean(skill_progression[skill_id][t])),
                    'count': len(skill_progression[skill_id][t])
                }
                for t in time_steps[:10]  # First 10 time steps as example
            ]
            statistics['skill_statistics'][int(skill_id)]['progression_sample'] = progression
    
    return statistics


def save_mastery_states_csv(mastery_data, output_path):
    """Save mastery states to CSV file."""
    
    # Flatten the data for CSV
    rows = []
    for entry in mastery_data:
        for skill_id, mastery_value in entry['mastery_states'].items():
            rows.append({
                'student_id': entry['student_id'],
                'time_step': entry['time_step'],
                'question_id': entry['question_id'],
                'skill_id': skill_id,
                'response': entry['response'],
                'mastery_state': mastery_value
            })
    
    # Write to CSV
    if rows:
        fieldnames = ['student_id', 'time_step', 'question_id', 'skill_id', 'response', 'mastery_state']
        with open(output_path, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
    
    return len(rows)


def main():
    parser = argparse.ArgumentParser(description='Extract mastery states from trained model')
    parser.add_argument('--run_dir', type=str, required=True, 
                       help='Experiment directory containing model checkpoint')
    parser.add_argument('--ckpt_name', type=str, default='model_best.pth',
                       help='Checkpoint filename (default: model_best.pth)')
    parser.add_argument('--split', type=str, default='test', choices=['train', 'valid', 'test'],
                       help='Data split to analyze (default: test)')
    parser.add_argument('--num_students', type=int, default=20,
                       help='Number of students to process (default: 20)')
    
    args = parser.parse_args()
    
    print("="*80)
    print("MASTERY STATES ANALYZER")
    print("="*80)
    print(f"Run directory: {args.run_dir}")
    print(f"Checkpoint: {args.ckpt_name}")
    print(f"Data split: {args.split}")
    print("="*80)
    
    # Load model and config
    print("\nüìä Loading model and configuration...")
    model, config, data_config, device, num_concepts = load_model_and_config(
        args.run_dir, args.ckpt_name
    )
    print(f"‚úì Model loaded successfully")
    print(f"‚úì Number of concepts: {num_concepts}")
    
    # Load data
    print(f"\nüìö Loading {args.split} data...")
    
    if args.split in ['train', 'valid']:
        # Use train/valid loaders
        train_loader, valid_loader = init_dataset4train(
            config['dataset'], 'gainakt4', data_config, config['fold'], config['batch_size']
        )
        data_loader = train_loader if args.split == 'train' else valid_loader
    else:
        # Load test data
        test_cfg = data_config[config['dataset']]
        test_dataset = KTDataset(
            os.path.join(test_cfg['dpath'], test_cfg['test_file']),
            test_cfg['input_type'],
            {-1}
        )
        data_loader = DataLoader(
            test_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=int(os.getenv('PYKT_NUM_WORKERS', '4')),
            pin_memory=True
        )
    
    print(f"‚úì Data loaded: {len(data_loader)} batches")
    
    # Extract mastery states
    print(f"\nüîç Extracting mastery states (max {args.num_students} students)...")
    mastery_data = extract_mastery_states(model, data_loader, device, num_concepts, max_students=args.num_students)
    
    # Check if mastery data is available
    if not mastery_data:
        print("\n" + "="*80)
        print("MASTERY STATES EXTRACTION SKIPPED")
        print("="*80)
        print("No mastery data available. This occurs when:")
        print("  - Model trained with Œª_bce=1.0 (pure BCE mode, Head 2 disabled)")
        print("  - No students in dataset (empty data loader)")
        print("\nNo output files created.")
        print("="*80)
        return
    
    print(f"‚úì Extracted {len(mastery_data)} observations from up to {args.num_students} students")
    
    # Compute statistics
    print("\nüìà Computing statistics...")
    statistics = compute_mastery_statistics(mastery_data, num_concepts)
    print(f"‚úì Analyzed {statistics['skills_observed']} skills")
    
    # Save results
    print("\nüíæ Saving results...")
    
    # Save CSV
    csv_path = os.path.join(args.run_dir, f'mastery_states_{args.split}.csv')
    num_rows = save_mastery_states_csv(mastery_data, csv_path)
    print(f"‚úì Saved {num_rows} rows to: {csv_path}")
    
    # Save statistics JSON
    stats_path = os.path.join(args.run_dir, f'mastery_states_summary_{args.split}.json')
    with open(stats_path, 'w') as f:
        json.dump(statistics, f, indent=2)
    print(f"‚úì Saved summary to: {stats_path}")
    
    # Print sample statistics
    print("\n" + "="*80)
    print("SUMMARY STATISTICS")
    print("="*80)
    print(f"Total observations: {statistics['total_observations']:,}")
    print(f"Skills observed: {statistics['skills_observed']}")
    print(f"\nSample skill statistics (first 5 skills):")
    
    for skill_id in sorted(statistics['skill_statistics'].keys())[:5]:
        stats = statistics['skill_statistics'][skill_id]
        print(f"\n  Skill {skill_id}:")
        print(f"    Count: {stats['count']}")
        print(f"    Mean mastery: {stats['mean']:.4f}")
        print(f"    Std: {stats['std']:.4f}")
        print(f"    Range: [{stats['min']:.4f}, {stats['max']:.4f}]")
    
    print("\n" + "="*80)
    print("‚úÖ Mastery states analysis completed successfully")
    print("="*80)


if __name__ == '__main__':
    main()
