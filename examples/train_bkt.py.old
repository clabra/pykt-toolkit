"""
Compute the mastery state of all the skills for the student at each interaction using pyBKT (Bayesian Knowledge Tracing)

Command: 
python train_bkt.py --dataset assist2015
python train_bkt.py --dataset assist2015 --prepare_data --output_path data/assist2015/bkt_mastery_states.pkl

This script uses pyBKT to:
1. Learn BKT parameters (P_L0, P_T, P_S, P_G) from training data
2. Run forward inference to compute P(learned) at each timestep
3. Save results 

Parameters: 
--dataset: str, required=True, help='Dataset name (e.g., assist2015)'
--output_path: str, default=None, help='Output path for targets (default: data/{dataset}/bkt_mastery_states.pkl, data/{dataset}/bkt_mastery_states_mono.pkl)')

Read data from: 
data_path = f'data/{args.dataset}/train_valid_sequences.csv

Output Files: 
- Standard version: data/[dataset_name]/bkt_mastery_states.pkl
- Monotonic version: data/[dataset_name]/bkt_mastery_states_mono.pkl

Model format (saved with pickle.dump): 
    {
        'bkt_mastery_states': bkt_mastery_states,
        'bkt_params': params,
        'metadata': {
            'dataset': args.dataset,
            'num_students': len(bkt_mastery_states),
            'num_skills': num_skills,
            'method': 'BKT (Bayesian Knowledge Tracing)',
            'model': 'pyBKT',
            'monotonic': False
        }
    }

    Where bkt_mastery_states is a dict with key=student_id, value=torch.Tensor[seq_len, num_skills] providing the mastery state of all the skills for the student at each interaction 
"""

import argparse
import pandas as pd
import numpy as np
import pickle
import torch
from pathlib import Path
import sys

# Add pyBKT to path
try:
    from pyBKT.models import Model
except ImportError:
    print("Error: pyBKT not installed. Run: pip install pyBKT")
    sys.exit(1)


def prepare_bkt_data(df):
    """
    Convert pykt sequence format to pyBKT format.
    
    Input: dataframe with uid (user_id), concepts (skill_name), responses (correct), selectmasks (order_id)
    Output: pyBKT format with user_id, skill_name, correct, order_id
    """
    records = []
    
    for idx, row in df.iterrows():
        uid = row['uid']
        concepts = [int(c) for c in row['concepts'].split(',') if c != '-1']
        responses = [int(r) for r in row['responses'].split(',') if r != '-1']
        selectmasks = [int(m) for m in row['selectmasks'].split(',') if m != '-1']
        
        # Convert to row-per-interaction format
        for order_id, (skill, response, mask) in enumerate(zip(concepts, responses, selectmasks)):
            if mask == 1:
                records.append({
                    'user_id': uid,
                    'skill_name': skill,
                    'correct': response,
                    'order_id': order_id
                })
        
        if (idx + 1) % 1000 == 0:
            print(f"  Prepared {idx + 1}/{len(df)} students...", flush=True)
    
    bkt_df = pd.DataFrame(records)
    return bkt_df


def fit_bkt_model(bkt_df, num_skills):
    """
    Fit BKT model to training data using EM algorithm.

    # Returns dictionary of parameters per skill
    params[skill_id] = {
        'prior': ...,   # P(L₀)
        'learns': ...,  # P(T)
        'slips': ...,   # P(S)
        'guesses': ...  # P(G)
    }
    """
    print(f"\n{'='*80}")
    print("FITTING BKT MODEL")
    print(f"{'='*80}")
    print(f"Training on {len(bkt_df)} interactions")
    print(f"Students: {bkt_df['user_id'].nunique()}")
    print(f"Skills: {num_skills}")
    
    # Initialize model
    model = Model(
        seed=42,
        num_fits=1,  # Number of EM restarts
        parallel=False
    )
    
    defaults = {
         'order_id': 'order_id', 
         'skill_name': 'skill_name', 
         'correct': 'correct', 
         'user_id': 'user_id'
         }

    # Fit model (learns parameters via EM)
    print("\nFitting BKT parameters via Expectation-Maximization...")
    model.fit(data=bkt_df, defaults=defaults)
    
    print("✓ BKT model fitted successfully")
    
    # Extract learned parameters
    params_df = model.params()
    
    # Convert to dict format: {skill: {param: value}}
    params = {}
    skills = params_df.index.get_level_values('skill').unique()
    
    for skill in skills:
        skill_int = int(skill)
        params[skill_int] = {
            'prior': params_df.loc[(skill, 'prior', 'default'), 'value'],
            'learns': params_df.loc[(skill, 'learns', 'default'), 'value'],
            'slips': params_df.loc[(skill, 'slips', 'default'), 'value'],
            'guesses': params_df.loc[(skill, 'guesses', 'default'), 'value'],
        }
    
    print(f"\n{'='*80}")
    print("LEARNED BKT PARAMETERS (averaged across skills)")
    print(f"{'='*80}")
    
    # Parameters are per-skill
    p_l0_mean = np.mean([params[skill]['prior'] for skill in params])
    p_t_mean = np.mean([params[skill]['learns'] for skill in params])
    p_s_mean = np.mean([params[skill]['slips'] for skill in params])
    p_g_mean = np.mean([params[skill]['guesses'] for skill in params])
    
    print(f"P(L0) - Prior knowledge:     {p_l0_mean:.4f}")
    print(f"P(T)  - Learning rate:       {p_t_mean:.4f}")
    print(f"P(S)  - Slip probability:    {p_s_mean:.4f}")
    print(f"P(G)  - Guess probability:   {p_g_mean:.4f}")
    
    return model, params


def enforce_monotonicity_skill_wise(targets_dict):
    """
    Enforce monotonicity for each skill independently.
    Once P(L) increases for a skill, it cannot decrease in future timesteps.
    
    Args:
        targets_dict: dict {uid: torch.Tensor [seq_len, num_skills]}
    
    Returns:
        smoothed_dict: dict with same structure, monotonic per skill
    """
    smoothed = {}
    
    for uid, target_matrix in targets_dict.items():
        seq_len, num_skills = target_matrix.shape
        smoothed_matrix = target_matrix.clone()
        
        # For each skill column independently
        for skill_id in range(num_skills):
            # Forward pass: ensure P(L)[t] >= P(L)[t-1]
            for t in range(1, seq_len):
                if smoothed_matrix[t, skill_id] < smoothed_matrix[t-1, skill_id]:
                    smoothed_matrix[t, skill_id] = smoothed_matrix[t-1, skill_id]
        
        smoothed[uid] = smoothed_matrix
    
    return smoothed


def estimate_bkt_mastery_states(df, bkt_df, model, params, num_skills):
    """
    Compute mastery targets using pyBKT's predict function and full tensor reconstruction.

    Args:
        df: pandas DataFrame with columns ['uid', 'concepts', 'responses', 'selectmasks']
        bkt_df: prepared dataframe for pyBKT
        model: BKT model with parameters
        params: BKT parameters
        num_skills: number of skills

    Returns:
        dict with per-student mastery matrices: {student_id: torch.Tensor[seq_len, num_skills]}
    """
    print(f"\n{'='*80}")
    print("COMPUTING BKT MASTERY TARGETS")
    print(f"{'='*80}")
    
    # 1. Use pyBKT to predict state probabilities
    print("Running pyBKT prediction (state estimation)...")
    preds_df = model.predict(data=bkt_df)
    
    # preds_df has: user_id, order_id, skill_name, state_predictions
    # We need to map this back to the [uid][seq_len][num_skills] tensor structure.
    # Note: bkt_df 'order_id' corresponds to the index in the original interaction sequence for that user.
    
    # Group predictions by user for efficient processing
    print("Grouping predictions by user...")
    preds_by_user = preds_df.groupby('user_id')
    
    bkt_mastery_states= {}
    processed_count = 0
    
    # Pre-calculate default priors for all skills
    default_priors = torch.tensor([
        params[s]['prior'] if s in params else 0.5 
        for s in range(num_skills)
    ], dtype=torch.float32)

    total_students = len(df)
    
    # Iterate over original dataframe to maintain correct sequence length and structure
    for idx, row in df.iterrows():
        uid = row['uid'] # uid is integer in df
        
        # Original sequence processing to know sequence length
        concepts = [int(c) for c in row['concepts'].split(',') if c != '-1']
        selectmasks = [int(m) for m in row['selectmasks'].split(',') if m != '-1']
        
        # Only interactions with selectmask=1 are in bkt_df and preds_df
        # The target tensor needs to cover ALL steps in the sequence (even those masked out?)
        # Convention: The output tensor usually aligns with the input sequence length.
        # But valid_idx logic in BKT preparation filtered out masked steps.
        # Let's align with the VALID interactions.
        
        valid_idx = [i for i in range(len(concepts)) if selectmasks[i] == 1]
        seq_len = len(valid_idx) # This matches the number of rows for this user in bkt_df
        
        target_tensor = torch.zeros(seq_len, num_skills, dtype=torch.float32)
        
        # Initialize with priors
        current_state = default_priors.clone()
        
        if uid in preds_by_user.groups:
            user_preds = preds_by_user.get_group(uid).sort_values('order_id')
            
            # user_preds should have exactly seq_len rows
            if len(user_preds) != seq_len:
                 # This might happen if prepare_bkt_data dropped something? 
                 # But valid_idx matches prepare_bkt_data logic.
                 # Let's trust ordering.
                 pass

            # Iterate through predictions and update state
            # order_id is 0-based index of valid interactions
            
            for _, pred_row in user_preds.iterrows():
                t = int(pred_row['order_id']) 
                skill_id = int(pred_row['skill_name'])
                new_mastery = float(pred_row['state_predictions'])
                
                # Update state for this skill
                current_state[skill_id] = new_mastery
                
                # Assign to tensor
                target_tensor[t] = current_state
        
        bkt_mastery_states[uid] = target_tensor
        
        processed_count += 1
        if processed_count % 1000 == 0:
            print(f"  Processed {processed_count}/{total_students} students...")

    print(f"✓ Computed BKT targets for {len(bkt_mastery_states)} students")
    
    return bkt_mastery_states


def main():
    parser = argparse.ArgumentParser(description='Compute mastery states using BKT')
    parser.add_argument('--dataset', type=str, required=True, help='Dataset name (e.g., assist2015)')
    parser.add_argument('--output_path', type=str, default=None, 
                       help='Output path for targets (default: data/{dataset}/bkt_mastery_states.pkl)')
    parser.add_argument('--prepare_data', action='store_true', default=False,
                       help='Whether to prepare BKT data (convert from sequences). Default False (load existent data).')
    
    args = parser.parse_args()
    
    # Paths
    data_path = f'data/{args.dataset}/train_valid_sequences.csv'
    if args.output_path is None:
        output_path = f'data/{args.dataset}/bkt_mastery_states.pkl'
    else:
        output_path = args.output_path
    
    print(f"\n{'='*80}")
    print("BKT TARGET COMPUTATION")
    print(f"{'='*80}")
    print(f"Dataset: {args.dataset}")
    print(f"Data path: {data_path}")
    print(f"Output path: {output_path}")
    
    # Load data
    print(f"\nLoading dataset from: {data_path}")
    df = pd.read_csv(data_path)
    print(f"Loaded {len(df)} student sequences")
    
    # Get number of skills
    all_skills = set()
    for concepts_str in df['concepts']:
        skills = [int(c) for c in concepts_str.split(',') if c != '-1']
        all_skills.update(skills)
    num_skills = len(all_skills)
    print(f"Number of unique skills: {num_skills}")
    
    # Prepare data for pyBKT
    bkt_data_path = f'data/{args.dataset}/bkt_df.pkl'
    
    if args.prepare_data:
        print("\nPreparing data for BKT...")
        bkt_df = prepare_bkt_data(df)
        print(f"Prepared {len(bkt_df)} interactions from {bkt_df['user_id'].nunique()} students")
        print(f"Saving prepared data to {bkt_data_path}")
        bkt_df.to_pickle(bkt_data_path)
    else:
        print(f"\nChecking for prepared data at {bkt_data_path}...")
        if Path(bkt_data_path).exists():
            print(f"Loading prepared data from {bkt_data_path}")
            bkt_df = pd.read_pickle(bkt_data_path)
            print(f"Loaded {len(bkt_df)} interactions")
        else:
             raise FileNotFoundError(f"Prepared BKT data not found at {bkt_data_path}. Run with --prepare_data to create it.")
    
    # Fit BKT model
    model, params = fit_bkt_model(bkt_df, num_skills)
    
    # Compute mastery states
    bkt_mastery_states= estimate_bkt_mastery_states(df, bkt_df, model, params, num_skills)
    
    # Apply monotonic smoothing
    print(f"\n{'='*80}")
    print("APPLYING MONOTONIC SMOOTHING")
    print(f"{'='*80}\n")
    bkt_mastery_states_mono = enforce_monotonicity_skill_wise(bkt_mastery_states)
    print(f"✓ Monotonic version created")
    
    # Save standard version
    print(f"\n{'='*80}")
    print("SAVING STANDARD VERSION")
    print(f"{'='*80}")
    
    result_standard = {
        'bkt_mastery_states': bkt_mastery_states,
        'bkt_params': params,
        'metadata': {
            'dataset': args.dataset,
            'num_students': len(bkt_mastery_states),
            'num_skills': num_skills,
            'method': 'BKT (Bayesian Knowledge Tracing)',
            'model': 'pyBKT',
            'monotonic': False
        }
    }
    
    with open(output_path, 'wb') as f:
        pickle.dump(result_standard, f)
    
    file_size_mb = Path(output_path).stat().st_size / (1024 * 1024)
    print(f"\n✓ Standard BKT targets saved to: {output_path}")
    print(f"  File size: {file_size_mb:.2f} MB")
    
    # Save monotonic version
    output_path_mono = output_path.replace('.pkl', '_mono.pkl')
    
    print(f"\n{'='*80}")
    print("SAVING MONOTONIC VERSION")
    print(f"{'='*80}")
    
    result_mono = {
        'bkt_mastery_states': bkt_mastery_states_mono,
        'bkt_params': params,
        'metadata': {
            'dataset': args.dataset,
            'num_students': len(bkt_mastery_states_mono),
            'num_skills': num_skills,
            'method': 'BKT (Bayesian Knowledge Tracing)',
            'model': 'pyBKT',
            'monotonic': True
        }
    }
    
    with open(output_path_mono, 'wb') as f:
        pickle.dump(result_mono, f)
    
    file_size_mb_mono = Path(output_path_mono).stat().st_size / (1024 * 1024)
    print(f"\n✓ Monotonic BKT targets saved to: {output_path_mono}")
    print(f"  File size: {file_size_mb_mono:.2f} MB")
    
    # Sample statistics
    sample_uid = list(bkt_mastery_states.keys())[0]
    sample_target = bkt_mastery_states[sample_uid]
    
    print(f"\n{'='*80}")
    print("BKT TARGET COMPUTATION COMPLETE")
    print(f"{'='*80}")
    print(f"Sample target tensor shape: {sample_target.shape}")
    print(f"Sample mastery values (first timestep): {sample_target[0, :10]}")
    print(f"\nGenerated files:")
    print(f"  1. {output_path} (standard, monotonic=False)")
    print(f"  2. {output_path_mono} (smoothed, monotonic=True)")

if __name__ == '__main__':
    main()
