# iKT3

Additional loss term for alignmet with the factor and prediction estimations of a theoretical model.  

```
Deprecated: 
Alignment failure with the reference model: All three alignment losses are poor
 
 Root Cause:
 The IRT reference model has poor predictive validity. Model prioritizes performance over aligning to bad reference
 ```

## Current Status (Dec 8, 2025)

**Implementation:** ‚úÖ Complete (single-phase training with IRT reference model)

**Performance:** Competitive with pykt baselines
- Test AUC: 0.7204 (ASSIST2015, fold 0, seed 42)
- Validation AUC: 0.7258 (best at epoch 7/30)
- Comparable to other pykt attention-based models (AKT, simpleKT)

**Baseline Experiment:** `20251208_191345_ikt3_baseline_286531`
- Configuration: Œª_target=0.05, warmup_epochs=50, c_stability=0.01
- Hyperparameters: d_model=256, n_heads=4, n_blocks=8, batch_size=64, lr=0.0001
- Reproducibility: ‚úÖ Verified (two independent runs produce identical results)

**Key Achievements:**
- ‚úÖ Dynamic IRT targets solve scale collapse (Œ∏_std: 0.14 ‚Üí 4.03, +2806%)
- ‚úÖ Adaptive lambda schedule implemented (Œª(t) = Œª_target √ó min(1, epoch/warmup))
- ‚úÖ Pluggable reference model architecture (IRT working, extensible to BKT)
- ‚úÖ Interpretability metrics validated (Œ∏, Œ≤ extracted successfully)
- ‚úÖ Benchmark infrastructure operational for model comparisons
- ‚úÖ Documentation complete with performance table in `paper/models.md`

**Critical Issues Identified:**
- ‚ùå **IRT alignment failure:** ALL three alignment losses far exceed thresholds
  - l_21 = 4.225 (threshold < 0.15, **28√ó over**) - M_IRT doesn't match M_ref
  - l_22 = 0.028 (threshold < 0.10, within limit but suboptimal) - Œ≤ doesn't match IRT calibration
  - l_23 = 6.929 (threshold < 0.15, **46√ó over**) - Œ∏ doesn't match IRT trajectories
- ‚ùå **Fundamental incompatibility:** Poor mastery-prediction correlation (r=0.022 Pearson)
- ‚ùå **Core paradox:** Model achieves decent prediction (AUC=0.72) but learns non-IRT factors

**Root Cause Analysis (CONFIRMED Dec 8, 2025):**
Investigation revealed IRT reference model has **poor predictive validity:**
1. ‚ùå **M_ref correlation: 0.19** (Pearson) - IRT predictions barely correlate with actual responses
2. ‚ùå **M_ref AUC: 0.63** - Only slightly better than random guessing
3. ‚ùå **Rasch model doesn't fit ASSIST2015** - Formula œÉ(Œ∏ - Œ≤) fundamentally incompatible with data
4. ‚úÖ **Lambda experiments validated incompatibility** - Tested Œª ‚àà [0.007, 0.15], l_21 always fails

**Why Alignment Fails:**
- l_21 = BCE(M_IRT, M_ref) tries to match model predictions to M_ref
- But M_ref itself is wrong (correlation=0.19 with ground truth)
- Model cannot align to bad targets no matter how high Œª is
- This explains why l_21 ‚âà 4+ across all Œª values tested
- **Model correctly prioritizes performance over aligning to bad reference**

**Interpretation:**
- Model demonstrates **learned interpretability** rather than theory-grounded alignment
- Œ∏, Œ≤ are **meaningful features** for prediction but not on IRT scale
- Performance comparable to other pykt models validates architecture
- Poor IRT alignment reflects dataset limitations, not model failure

**Next Steps:**
- Compare with other pykt models (AKT, DKT, SAKT) using benchmark infrastructure
- Investigate alternative reference models (BKT, DINA) that may fit ASSIST2015 better
- Consider dataset-specific calibration or multi-dataset validation
- Document model comparison results for paper 

## Architecture 

```mermaid
graph TD
    subgraph "Input Layer"
        Input_q[["Input Questions q<br/>[B, L]"]]
        Input_r[["Input Responses r<br/>[B, L]"]]
        Ground_Truth_r[["Ground Truth Responses<br/>[B, L]"]]
    end
    
    subgraph "Reference Model Targets (Pre-computed)"
        RefTargets[["IRT Reference Targets<br/>Œ≤_IRT: [num_skills]<br/>Œ∏_IRT: {uid: scalar}<br/>M_ref: {uid: [seq_len]}"]]
    end
    
    subgraph "Embedding Layer"
        Tokens[["Interaction Tokens<br/>(q, r) pairs"]]
        Context_Emb["Context Embedding<br/>[B, L, d_model]"]
        Value_Emb["Value Embedding<br/>[B, L, d_model]"]
        Skill_Emb["Skill Embedding<br/>[B, L, d_model]"]
        Pos_Emb["Positional Embeddings<br/>[1, L, d_model]"]
        
        Context_Seq[["Context Sequence<br/>c = emb(q,r) + pos<br/>[B, L, d_model]"]]
        Value_Seq[["Value Sequence<br/>v = emb(r) + pos<br/>[B, L, d_model]"]]
    end
    
    subgraph "Transformer Encoder"
        direction TB
        
        subgraph "Context Stream Attention"
            Q_c["Q_context = Linear(c)<br/>[B, L, d_model]"]
            K_c["K_context = Linear(c)<br/>[B, L, d_model]"]
            V_c["V_context = Linear(c)<br/>[B, L, d_model]"]
            Attn_c["Multi-Head Attention<br/>softmax(QK^T/‚àöd)V"]
            Out_c["h' = Attention(Q_c, K_c, V_c)<br/>[B, L, d_model]"]
        end
        
        subgraph "Value Stream Attention"
            Q_v["Q_value = Linear(v)<br/>[B, L, d_model]"]
            K_v["K_value = Linear(v)<br/>[B, L, d_model]"]
            V_v["V_value = Linear(v)<br/>[B, L, d_model]"]
            Attn_v["Multi-Head Attention<br/>softmax(QK^T/‚àöd)V"]
            Out_v["v' = Attention(Q_v, K_v, V_v)<br/>[B, L, d_model]"]
        end
        
        FFN_C["Feed-Forward Context<br/>√ó N blocks"]
        FFN_V["Feed-Forward Value<br/>√ó N blocks"]
        
        Note_Stack["√ó N Transformer Blocks<br/>(N=8, d_model=256)"]
    end
    
    subgraph "Encoder Output"
        Final_h[["Knowledge State h<br/>(final context)<br/>[B, L, d_model]"]]
        Final_v[["Value State v<br/>(final value)<br/>[B, L, d_model]"]]
    end
    
    subgraph "Head 1: Performance Prediction (BCE)"
        Concat1["Concat[h, v, skill_emb]<br/>[B, L, 3¬∑d_model]"]
        PredHead["MLP Prediction Head<br/>Linear ‚Üí ReLU ‚Üí Dropout ‚Üí Linear"]
        Logits[["Logits<br/>[B, L]"]]
        BCEPred[["p_correct = œÉ(logits)<br/>[B, L]"]]
    end
    
    subgraph "Head 2: IRT-Based Mastery (Pluggable Reference Model)"
        direction TB
        
        AbilityEnc["Ability Encoder<br/>h ‚Üí Linear(d_ff) ‚Üí ReLU<br/>‚Üí Dropout ‚Üí Linear(1)"]
        Theta[["Œ∏_learned(t)<br/>Student Ability<br/>[B, L] scalars"]]
        
        SkillDiffEmb["Skill Difficulty Embeddings<br/>Œ≤_learned ~ Embedding(num_skills, 1)<br/>Initialized to 0.0"]
        Beta[["Œ≤_learned(k)<br/>Skill Difficulty<br/>[B, L] scalars"]]
        
        IRTFormula["IRT Rasch Formula<br/>M_IRT = œÉ(Œ∏_learned - Œ≤_learned)"]
        MasteryIRT[["M_IRT<br/>IRT-based Mastery<br/>[B, L] probabilities"]]
    end
    
    subgraph "Loss Computation (via Reference Model Interface)"
        direction LR
        
        L_BCE["l_bce<br/>BCE(p_correct, targets)<br/>Performance Loss"]
        
        L_21["l_21 (performance)<br/>BCE(M_IRT, M_ref)<br/>Prediction alignment"]
        
        L_22["l_22 (difficulty)<br/>MSE(Œ≤_learned[q], Œ≤_IRT[q])<br/>Difficulty regularization<br/>(always active)"]
        
        L_23["l_23 (ability)<br/>MSE(mean(Œ∏_learned), Œ∏_IRT)<br/>Ability alignment"]
        
        LambdaSchedule["Œª(epoch) Warm-up<br/>Œª = Œª_target √ó min(1, epoch/warmup)<br/>Œª_target=0.5, warmup=50"]
    end
    
    subgraph "Combined Loss (Single-Phase Training)"
        LTotal["L_total = (1-Œª)√ól_bce + c√ól_22 + Œª√ó(l_21 + l_23)<br/><br/>Œª: interpretability weight (warm-up)<br/>c: stability regularization (fixed, c=0.01)"]
        Backprop["Backpropagation<br/>Updates: Œ∏ encoder, Œ≤ embeddings,<br/>prediction head, encoder"]
    end
    
    %% Input to Embedding flow
    Input_q --> Tokens
    Input_r --> Tokens
    Tokens --> Context_Emb
    Tokens --> Value_Emb
    Input_q --> Skill_Emb
    Input_q --> SkillDiffEmb
    
    Context_Emb --> Context_Seq
    Value_Emb --> Value_Seq
    Pos_Emb --> Context_Seq
    Pos_Emb --> Value_Seq
    
    %% Encoder processing - Context stream
    Context_Seq --> Q_c
    Context_Seq --> K_c
    Context_Seq --> V_c
    Q_c --> Attn_c
    K_c --> Attn_c
    V_c --> Attn_c
    Attn_c --> Out_c
    Out_c --> FFN_C
    
    %% Encoder processing - Value stream
    Value_Seq --> Q_v
    Value_Seq --> K_v
    Value_Seq --> V_v
    Q_v --> Attn_v
    K_v --> Attn_v
    V_v --> Attn_v
    Attn_v --> Out_v
    Out_v --> FFN_V
    
    %% Final encoder outputs
    FFN_C --> Final_h
    FFN_V --> Final_v
    
    %% Head 1 - Performance prediction
    Final_h --> Concat1
    Final_v --> Concat1
    Skill_Emb --> Concat1
    Concat1 --> PredHead --> Logits --> BCEPred
    
    %% Head 2 - IRT mastery inference
    Final_h --> AbilityEnc --> Theta
    SkillDiffEmb --> Beta
    Theta --> IRTFormula
    Beta --> IRTFormula
    IRTFormula --> MasteryIRT
    
    %% Loss computation flows
    BCEPred --> L_BCE
    Ground_Truth_r --> L_BCE
    
    MasteryIRT --> L_21
    RefTargets --> L_21
    
    Beta --> L_22
    RefTargets --> L_22
    
    Theta --> L_23
    RefTargets --> L_23
    
    %% Loss aggregation with lambda schedule
    L_BCE --> LTotal
    L_21 --> LTotal
    L_22 --> LTotal
    L_23 --> LTotal
    LambdaSchedule --> LTotal
    
    LTotal --> Backprop
    
    %% Gradient flow (dotted lines)
    Backprop -.->|‚àÇL/‚àÇh| Final_h
    Backprop -.->|‚àÇL/‚àÇv| Final_v
    Backprop -.->|‚àÇL/‚àÇŒ≤| SkillDiffEmb
    Backprop -.->|‚àÇL/‚àÇŒ∏| AbilityEnc
    
    %% Styling
    classDef input_style fill:#ffffff,stroke:#333333,stroke-width:2px
    classDef ref_style fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef emb_style fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef encoder_style fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef head1_style fill:#c8e6c9,stroke:#388e3c,stroke-width:3px
    classDef head2_style fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    classDef loss_style fill:#e1bee7,stroke:#7b1fa2,stroke-width:3px
    classDef combined_style fill:#f3e5f5,stroke:#6a1b9a,stroke-width:4px
    
    class Input_q,Input_r,Ground_Truth_r input_style
    class RefTargets ref_style
    class Tokens,Context_Emb,Value_Emb,Skill_Emb,Pos_Emb,Context_Seq,Value_Seq emb_style
    class Q_c,K_c,V_c,Attn_c,Out_c,Q_v,K_v,V_v,Attn_v,Out_v,FFN_C,FFN_V,Note_Stack,Final_h,Final_v encoder_style
    class Concat1,PredHead,Logits,BCEPred head1_style
    class AbilityEnc,Theta,SkillDiffEmb,Beta,IRTFormula,MasteryIRT head2_style
    class L_BCE,L_21,L_22,L_23,LambdaSchedule loss_style
    class LTotal,Backprop combined_style
```
## Loss Formulation

In Head 2, we measure consistency with the reference theoretical model through three losses:

1) **Prediction alignment** (l_21): Head 2 mastery prediction (mastery_irt) <-> IRT precalculated performance probability (BCE)
2) **Difficulty regularization** (l_22): Head 2 beta value <-> IRT precalculated difficulty (MSE)
3) **Ability alignment** (l_23): Head 2 theta value <-> IRT precalculated ability (MSE)

**Combined loss:**
```
L = (1-Œª) √ó l_bce + c √ó l_22 + Œª √ó (l_21 + l_23)
```

**Key insight:** l_22 is **not** controlled by Œª because it serves a different purpose:
- **l_22 (difficulty regularization)**: Always active, prevents drift from pre-calibrated Œ≤ values (stability constraint)
- **l_21, l_23 (interpretability alignment)**: Controlled by Œª, trades off with performance (interpretability objective)

**Why Œ≤ regularization should be active even when interpretability is not prioritized:**
1. **Œ≤ is pre-calibrated**: These are stable, dataset-level properties derived from IRT calibration that shouldn't drift during training
2. **Prevents mode collapse**: Without Œ≤ anchoring, the model might learn degenerate solutions where difficulty embeddings collapse to arbitrary values
3. **Different purpose**: Œ≤ regularization is about **stability/validity** of the model, not about the interpretability trade-off‚Äîit ensures the model remains theoretically grounded regardless of Œª

**Parameters:**
- **Œª ‚àà [0,1]**: Single interpretability trade-off parameter
  - Œª = 0: Pure performance optimization (l_bce) + difficulty stability (c√ól_22)
  - Œª = 1: Full IRT consistency enforcement (all alignment losses active)
  - Can follow warm-up schedule (starting low, gradually increasing)
  
- **c**: Fixed difficulty regularization weight (independent of Œª)
  - Suggested value: c = 0.01 (gentle regularization, always active)
  - Rationale: Œ≤ values are pre-calibrated from IRT and should remain stable throughout training, regardless of interpretability priority
  - Purpose: Prevents mode collapse and maintains consistency with dataset-level difficulty calibration
  
**Design Rationale:**
- **Separation of concerns**: 
  - Difficulty regularization (c√ól_22) is about **validity/stability** of pre-calibrated anchors
  - Interpretability alignment (Œª√ó(l_21 + l_23)) is about **learning theory-consistent factors**
- **Single parameter for analysis**: Œª controls the performance-interpretability trade-off for Pareto curves
- **Always-on regularization**: Even when Œª = 0 (pure performance mode), l_22 keeps Œ≤ values anchored to IRT calibration

## Rationale

We aim to infer factor values (Œ∏, Œ≤) that are consistent with the theoretical model. Beta values (skill difficulties) are pre-calibrated from the dataset and represent stable item properties. Therefore:

- **l_22 acts as an always-on regularization**: Prevents Œ≤ from drifting away from pre-calibrated values, maintaining theoretical validity
- **l_21 and l_23 are interpretability objectives**: Controlled by Œª to balance performance vs theory-consistent factor learning

If we imagine Head 2 as a box trying to replicate the theoretical model, then we need to replicate both the box (the IRT formula) and its inputs (Œ∏, Œ≤). However, Œ≤ is pre-calibrated and should be treated as a stability constraint, while Œ∏ and performance predictions are dynamically learned and controlled by the interpretability parameter Œª. 

```mermaid
graph LR
    theta["Œ∏ (ability)"] --> IRT
    beta["Œ≤ (difficulty)"] --> IRT
    
    subgraph IRT["Head 2 - IRT Model"]
        formula["M = œÉ(Œ∏ - Œ≤)"]
    end
    
    IRT --> M["M (mastery probability)"]
```


## Hypotheses for Construct Validity (Loss-Based Formulation)

To demonstrate that Head 2's learned factors represent valid IRT constructs, we formulate hypotheses in terms of **actionable alignment losses** that both validate constructs and steer the system toward theory-consistent states.

### Minimal Validation Set (3 Hypotheses)

---

### H1: Factor Alignment (via l_22 and l_23)

**Hypothesis**: Minimizing alignment losses l_22 and l_23 drives learned factors toward IRT-calibrated values, establishing convergent validity.

**Loss Formulation**:
```
l_22 = MSE(Œ≤_learned, Œ≤_IRT)  # Difficulty alignment
l_23 = MSE(Œ∏_learned, Œ∏_IRT)  # Ability alignment
```

**Validation Criterion**: 
- l_22 < 0.10 (low MSE between learned and IRT difficulties)
- l_23 < 0.15 (low MSE between learned and IRT abilities, slightly higher threshold due to temporal dynamics)

**Interpretation**: 
- Low l_22 ‚Üí Œ≤_learned ‚âà Œ≤_IRT ‚Üí model learns correct difficulty ordering
- Low l_23 ‚Üí Œ∏_learned ‚âà Œ∏_IRT ‚Üí model learns ability values consistent with psychometric calibration

**Actionable**: These losses directly optimize alignment during training. If validation fails (high MSE), increase Œª‚ÇÇ or Œª weights in combined loss.

---

### H2: Predictive Consistency (via l_21)

**Hypothesis**: Minimizing l_21 ensures Head 2's IRT-based predictions match reference IRT model, validating that the learned IRT mechanism is functionally equivalent to theory.

**Loss Formulation**:
```
l_21 = BCE(M_IRT, M_ref)

where:
  M_IRT = œÉ(Œ∏_learned - Œ≤_learned)  # Head 2's prediction
  M_ref = œÉ(Œ∏_IRT - Œ≤_IRT)          # Reference IRT prediction
```

**Validation Criterion**: 
- l_21 < 0.15 (low cross-entropy between Head 2 and reference IRT)

**Interpretation**: Low l_21 means that even if individual factors have small errors, their **combination through the IRT formula** produces correct predictions. This validates the entire IRT mechanism, not just individual components.

**Actionable**: Directly optimizable. If l_21 remains high despite low l_22/l_23, this indicates formula misapplication (implementation bug) rather than alignment failure.

---

### H3: Integration Validation (monitoring val_heads_corr)

**Hypothesis**: Head 1 (data-driven) and Head 2 (theory-driven) produce compatible predictions when alignment losses are minimized, confirming successful integration of performance and interpretability.

**Metric** (monitoring, not directly optimized):
```
val_heads_corr = corr(p_correct, M_IRT) > 0.85
```

**Interpretation**: High correlation emerges as a consequence of minimizing L_align = MSE(p_correct, M_IRT) in Phase 2. This validates that:
- Head 1 learns predictive patterns compatible with IRT theory
- Head 2 produces interpretable estimates that preserve predictive power
- The dual-head architecture successfully balances accuracy and interpretability

**Not Directly Actionable**: This is an emergent property, not an optimization target. If correlation is low despite low alignment losses, it indicates architectural issues (e.g., insufficient model capacity).

---

## Implementation Strategy

### Overview: Current Implementation Status

iKT3 has been implemented as a **new standalone model** with pluggable reference model architecture. The implementation follows the design principle of separating the neural architecture from theoretical grounding, allowing multiple reference models (IRT, BKT, future models) to be used interchangeably.

**Implementation Status:** ‚úÖ **COMPLETE** for IRT reference model

**Key Files:**
- `pykt/models/ikt3.py` - Core model implementation
- `pykt/reference_models/base.py` - Abstract reference model interface
- `pykt/reference_models/irt_reference.py` - IRT reference implementation
- `examples/train_ikt3.py` - Training script with warm-up schedule
- `examples/eval_ikt3.py` - Evaluation with correlation diagnostics
- `examples/compute_irt_extended_targets.py` - IRT target generation

---

### 1. Reference Model Architecture (Implemented)

#### Abstract Base Class (`pykt/reference_models/base.py`)

Defines the interface for all reference models:

```python
class ReferenceModel(ABC):
    def __init__(self, model_name: str, num_skills: int)
    
    @abstractmethod
    def load_targets(self, targets_path: str) -> Dict[str, torch.Tensor]:
        """Load pre-computed reference targets from file"""
    
    @abstractmethod
    def compute_alignment_losses(
        self, 
        model_outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor],
        lambda_weights: Dict[str, float]
    ) -> Dict[str, torch.Tensor]:
        """Compute model-specific alignment losses"""
    
    @abstractmethod
    def get_loss_names(self) -> List[str]:
        """Return list of loss component names for logging"""
    
    @abstractmethod
    def get_interpretable_factors(
        self, 
        model_outputs: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """Extract interpretable factors for validation"""
```

**Design Benefits:**
- Single interface for all reference models
- Clean separation of concerns
- Easy to add new reference models (2-3 days per model)
- Enables A/B testing between different theoretical groundings

---

### 2. IRT Reference Model (Implemented)

#### Implementation: `pykt/reference_models/irt_reference.py`

**Initialization:**
```python
class IRTReferenceModel(ReferenceModel):
    def __init__(self, num_skills: int):
        super().__init__("IRT", num_skills)
```

#### Target Loading

Loads pre-computed IRT calibration from pickle file:

```python
def load_targets(self, targets_path: str) -> Dict[str, torch.Tensor]:
    """
    Loads:
    - beta_irt: [num_skills] - IRT-calibrated skill difficulties
    - theta_irt: {uid: float} - Student abilities (dict)
    - m_ref: {uid: [seq_len]} - Reference predictions œÉ(Œ∏ - Œ≤)
    - metadata: Dataset information
    """
```

**Target File Structure:**
```python
{
    'skill_difficulties': {0: Œ≤_0, 1: Œ≤_1, ..., 99: Œ≤_99},  # IRT difficulties
    'student_abilities': {uid_0: Œ∏_0, uid_1: Œ∏_1, ...},     # Student abilities
    'reference_predictions': {uid_0: [M_0, M_1, ...], ...}, # œÉ(Œ∏ - Œ≤) sequences
    'metadata': {
        'num_skills': 100,
        'num_students': 15426,
        'num_sequences': 12220,
        'theta_mean': -0.015,
        'theta_std': 1.023,
        'beta_mean': 0.002,
        'beta_std': 0.987
    }
}
```

#### Loss Computation

**Three alignment losses computed:**

1. **l_21 (Performance Alignment):**
```python
l_21 = F.binary_cross_entropy(
    model_outputs['mastery_irt'],  # Model's œÉ(Œ∏_learned - Œ≤_learned)
    targets['m_ref'],               # Reference œÉ(Œ∏_IRT - Œ≤_IRT)
    reduction='mean'
)
```

**What it measures:** Prediction consistency between learned and reference IRT models

**Success threshold:** l_21 < 0.15

2. **l_22 (Difficulty Regularization):**
```python
# Extract IRT difficulties for specific skills in batch
beta_irt_full = targets['beta_irt']     # [num_skills] all difficulties
questions = model_outputs['questions']   # [B, L] skill indices
beta_irt_batch = beta_irt_full[questions]  # [B, L] batch-specific

l_22 = F.mse_loss(
    model_outputs['beta_learned'],  # Learned embeddings [B, L]
    beta_irt_batch,                 # Reference difficulties [B, L]
    reduction='mean'
)
```

**What it measures:** How well learned difficulty embeddings match IRT calibration

**Success threshold:** l_22 < 0.10

3. **l_23 (Ability Alignment):**
```python
# Average student ability over sequence for MSE
theta_learned_mean = model_outputs['theta_t'].mean(dim=1)  # [B]
theta_irt_batch = torch.stack([
    targets['theta_irt'][uid] for uid in batch_uids
])  # [B]

l_23 = F.mse_loss(
    theta_learned_mean,
    theta_irt_batch,
    reduction='mean'
)
```

**What it measures:** How well learned ability estimates match IRT calibration

**Success threshold:** l_23 < 0.15

**Combined Output:**
```python
return {
    'l_21_performance': l_21,
    'l_22_difficulty': l_22,
    'l_23_ability': l_23,
    'l_align_total': l_21 + l_23  # For Œª weighting in total loss
}
```

#### Interpretable Factor Extraction

```python
def get_interpretable_factors(self, model_outputs):
    return {
        'theta': model_outputs['theta_t'],      # [B, L] ability trajectories
        'beta': model_outputs['beta_k'],        # [B, L] difficulty values
        'mastery': model_outputs['mastery_irt'] # [B, L] IRT predictions
    }
```

Used for:
- Validation metrics (correlation with reference)
- Visualization and interpretation
- Case study analysis

---

### 3. IRT Target Generation (Implemented)

#### Script: `examples/compute_irt_extended_targets.py`

**Purpose:** Pre-compute IRT calibration targets for training

**Process:**

1. **Load Existing Rasch Difficulties:**
```python
# From rasch_test_iter300.pkl (already calibrated)
skill_difficulties = {0: Œ≤_0, 1: Œ≤_1, ..., 99: Œ≤_99}
```

2. **Compute Student Abilities:**

Uses simplified IRT calibration on train + validation data:

```python
For each student i:
    interactions = [(question_j, response_ij), ...]
    
    # Estimate Œ∏ via maximum likelihood
    Œ∏_i = mean(logit(response_ij) + Œ≤_j) over all interactions
    
    # Alternative: Use proper MLE with optimization
    Œ∏_i = argmax_Œ∏ ‚àè_j P(response_ij | Œ∏, Œ≤_j)
```

**Current implementation:** Uses averaging method for computational efficiency

3. **Generate Reference Predictions:**

For each student-question pair:
```python
M_ref[i, j] = œÉ(Œ∏_i - Œ≤_j)  # Rasch 1PL formula
```

4. **Save Extended Targets:**
```bash
python examples/compute_irt_extended_targets.py \
    --dataset assist2015 \
    --fold 0 \
    --rasch_path data/assist2015/rasch_test_iter300.pkl \
    --output_path data/assist2015/irt_extended_targets_fold0.pkl
```

**Output:** `irt_extended_targets_fold0.pkl` with structure shown above

**Status:** ‚úÖ Generated for ASSIST2015 fold 0, ready for all folds

---

### 4. Model Architecture (Implemented)

#### Core Model: `pykt/models/ikt3.py`

**Key Components:**

1. **Initialization with Reference Model Type:**
```python
class iKT3(nn.Module):
    def __init__(
        self,
        num_c, seq_len, d_model, n_heads, num_encoder_blocks,
        d_ff, dropout, emb_type,
        reference_model_type='irt'  # Determines head architecture
    ):
        # Shared encoder (same as iKT2)
        self._init_embeddings()
        self._init_encoder()
        
        # Head 1: Performance prediction (BCE)
        self._init_performance_head()
        
        # Head 2: Reference-model-specific (IRT, BKT, etc.)
        if reference_model_type == 'irt':
            self._init_irt_heads()
        elif reference_model_type == 'bkt':
            self._init_bkt_heads()  # Future
        
        self.reference_model = None  # Injected later
        self.reference_model_type = reference_model_type
```

2. **IRT Head Initialization:**
```python
def _init_irt_heads(self, d_ff, dropout):
    # Ability encoder: h ‚Üí Œ∏
    self.ability_encoder = nn.Sequential(
        nn.Linear(self.d_model, d_ff),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(d_ff, 1)
    )
    
    # Difficulty embeddings: skill_id ‚Üí Œ≤
    self.skill_difficulty_emb = nn.Embedding(self.num_c, 1)
    nn.init.constant_(self.skill_difficulty_emb.weight, 0.0)  # Neutral init
```

3. **Forward Pass (IRT-specific):**
```python
def _forward_irt(self, h, v, qry):
    """
    Args:
        h: Knowledge state [B, L, d_model]
        v: Value state [B, L, d_model]
        qry: Query questions [B, L]
    
    Returns:
        Dictionary with predictions and interpretable factors
    """
    # Extract ability from knowledge state
    theta_t = self.ability_encoder(h).squeeze(-1)  # [B, L]
    
    # Extract difficulty embeddings
    beta_k = self.skill_difficulty_emb(qry).squeeze(-1)  # [B, L]
    
    # IRT formula
    mastery_irt = torch.sigmoid(theta_t - beta_k)  # [B, L]
    
    # Performance prediction (Head 1)
    combined = torch.cat([h, v, self.question_emb(qry)], dim=-1)
    logits = self.performance_head(combined).squeeze(-1)
    bce_predictions = torch.sigmoid(logits)
    
    return {
        'logits': logits,
        'bce_predictions': bce_predictions,
        'mastery_irt': mastery_irt,
        'theta_t': theta_t,
        'beta_k': beta_k,
        'beta_learned': self.skill_difficulty_emb.weight.squeeze(),
        'questions': qry
    }
```

4. **Loss Computation (Delegated to Reference Model):**
```python
def compute_loss(
    self, 
    output, 
    targets, 
    ref_targets, 
    lambda_interp, 
    lambda_reg
):
    """
    L = (1-Œª) √ó l_bce + c √ó l_22 + Œª √ó (l_21 + l_23)
    """
    # Performance loss
    l_bce = F.binary_cross_entropy_with_logits(
        output['logits'], 
        targets
    )
    
    # Reference model alignment losses
    alignment_losses = self.reference_model.compute_alignment_losses(
        model_outputs=output,
        targets=ref_targets,
        lambda_weights={'lambda_interp': lambda_interp}
    )
    
    # Extract components
    l_stability = alignment_losses['l_22_difficulty']  # IRT
    l_align_total = alignment_losses['l_align_total']  # l_21 + l_23
    
    # Combined loss
    total_loss = (
        (1 - lambda_interp) * l_bce +
        lambda_reg * l_stability +
        lambda_interp * l_align_total
    )
    
    return {
        'total_loss': total_loss,
        'l_bce': l_bce,
        **alignment_losses
    }
```

---

### 5. Training Script (Implemented)

#### Script: `examples/train_ikt3.py`

**Key Features:**

1. **Reference Model Injection:**
```python
from pykt.reference_models import create_reference_model

# Create model
model = iKT3(
    num_c=num_skills,
    reference_model_type=args.reference_model  # 'irt'
)

# Inject reference model
ref_model = create_reference_model(args.reference_model, num_skills)
model.set_reference_model(ref_model)

# Load reference targets
ref_targets = ref_model.load_targets(args.reference_targets_path)
```

2. **Lambda Warm-up Schedule:**
```python
def get_lambda_interp(epoch, lambda_target, warmup_epochs):
    """
    Gradual warm-up: Œª(epoch) = Œª_target √ó min(1, epoch / warmup_epochs)
    
    Example: Œª_target = 0.5, warmup = 50
    - Epoch 1: Œª = 0.01 (2%)
    - Epoch 25: Œª = 0.25 (50%)
    - Epoch 50: Œª = 0.50 (100%)
    - Epoch 60: Œª = 0.50 (capped)
    """
    return lambda_target * min(1.0, epoch / warmup_epochs)
```

3. **Training Loop:**
```python
for epoch in range(1, epochs + 1):
    lambda_current = get_lambda_interp(epoch, lambda_target, warmup_epochs)
    
    for batch in train_loader:
        # Prepare batch-specific reference targets
        batch_ref_targets = prepare_batch_ref_targets(
            batch, ref_targets, device
        )
        
        # Forward pass
        outputs = model(q, r, qry)
        
        # Compute loss
        loss_dict = model.compute_loss(
            output=outputs,
            targets=targets,
            ref_targets=batch_ref_targets,
            lambda_interp=lambda_current,
            lambda_reg=c_stability_reg
        )
        
        # Backward
        loss_dict['total_loss'].backward()
        optimizer.step()
```

4. **Metrics Logging:**
```python
# CSV tracking
metrics_epoch.csv:
    epoch, lambda_interp, 
    train_loss, train_auc, train_acc,
    val_loss, val_auc, val_acc,
    val_l_bce, val_l_stability, val_l_align_total,
    val_l_21_performance, val_l_22_difficulty, val_l_23_ability,
    val_theta_mean, val_theta_std,
    val_mastery_mean, val_mastery_std

metrics_valid.csv:
    split, auc, acc, 
    l_21_performance, l_22_difficulty, l_23_ability
```

**Status:** ‚úÖ Fully implemented and tested

---

### 6. Evaluation Script (Implemented)

#### Script: `examples/eval_ikt3.py`

**Key Features:**

1. **Comprehensive Metrics:**
```python
def evaluate_model(model, test_loader, ref_targets, device):
    metrics = {
        # Performance
        'auc': ...,
        'acc': ...,
        
        # Alignment losses
        'l_21_performance': ...,
        'l_22_difficulty': ...,
        'l_23_ability': ...,
        
        # Factor statistics
        'theta_mean': ...,
        'theta_std': ...,
        'theta_min': ...,
        'theta_max': ...,
        'mastery_mean': ...,
        'mastery_std': ...,
        
        # Correlations (DIAGNOSTIC)
        'mastery_prediction_pearson': ...,   # M_IRT vs M_ref
        'mastery_prediction_spearman': ...,
        'theta_pearson': ...,                # Œ∏_learned vs Œ∏_IRT
        'beta_pearson': ...                  # Œ≤_learned vs Œ≤_IRT
    }
```

2. **Correlation Diagnostic (NEW):**
```python
# Critical for diagnosing l_21 paradox
mastery_irt_arr = np.array(all_mastery_irt)
mastery_ref_arr = np.array(all_mastery_ref)

# Remove NaN/inf
valid_mask = np.isfinite(mastery_irt_arr) & np.isfinite(mastery_ref_arr)
mastery_irt_clean = mastery_irt_arr[valid_mask]
mastery_ref_clean = mastery_ref_arr[valid_mask]

# Compute correlations
pearson_corr, _ = pearsonr(mastery_irt_clean, mastery_ref_clean)
spearman_corr, _ = spearmanr(mastery_irt_clean, mastery_ref_clean)

print(f"Mastery Prediction Correlation")
print(f"Pearson:  {pearson_corr:.4f}")
print(f"Spearman: {spearman_corr:.4f}")

# Interpretation:
# - corr > 0.85: Good alignment, l_21 issue is scale/offset
# - corr 0.5-0.85: Partial alignment, needs more training
# - corr < 0.5: Fundamental disagreement, architecture issue
```

3. **Success Criteria Check:**
```python
def check_success_criteria(metrics, reference_model_type):
    if reference_model_type == 'irt':
        criteria = {
            'l_21_performance': {
                'value': metrics['l_21_performance'],
                'threshold': 0.15,
                'passed': metrics['l_21_performance'] < 0.15
            },
            'l_22_difficulty': {
                'value': metrics['l_22_difficulty'],
                'threshold': 0.10,
                'passed': metrics['l_22_difficulty'] < 0.10
            },
            'l_23_ability': {
                'value': metrics['l_23_ability'],
                'threshold': 0.15,
                'passed': metrics['l_23_ability'] < 0.15
            },
            'mastery_correlation': {
                'value': metrics.get('mastery_prediction_pearson', 0.0),
                'threshold': 0.85,
                'passed': metrics.get('mastery_prediction_pearson', 0.0) > 0.85
            }
        }
        
        overall = all(c['passed'] for c in criteria.values())
        criteria['overall'] = overall
        
    return criteria
```

**Status:** ‚úÖ Fully implemented with correlation diagnostics

---

### 7. Current Implementation Gaps

#### Implemented ‚úÖ
- IRT reference model with all three alignment losses
- Pluggable reference model architecture (base class)
- Target generation for IRT (extended targets)
- Training with lambda warm-up
- Evaluation with correlation diagnostics
- Reproducibility framework integration

#### Not Yet Implemented ‚è≥
- BKT reference model implementation
- BKT target generation script
- DINA, PFA, AFM reference models (future)
- Hybrid reference models
- Multi-reference comparison tools

#### Known Issues üêõ
- **Alignment Paradox:** Low MSE (l_22, l_23) but high BCE (l_21) with low correlation
  - Root cause: Insufficient training (Œª only 60% of warm-up) + weak regularization (c = 0.01)
  - Solution: Train 60+ epochs with c = 0.1
  - See "Observed Problem" section for details

---

### 8. Usage Example

**Complete workflow:**

```bash
# 1. Generate IRT targets
python examples/compute_irt_extended_targets.py \
    --dataset assist2015 \
    --fold 0 \
    --rasch_path data/assist2015/rasch_test_iter300.pkl \
    --output_path data/assist2015/irt_extended_targets_fold0.pkl

# 2. Train model
python examples/run_repro_experiment.py \
    --model ikt3 \
    --dataset assist2015 \
    --fold 0 \
    --reference_model irt \
    --reference_targets_path data/assist2015/irt_extended_targets_fold0.pkl \
    --epochs 60 \
    --lambda_target 0.5 \
    --warmup_epochs 50 \
    --c_stability_reg 0.1 \
    --short_title "ikt3_irt_v2"

# 3. Evaluate
python examples/eval_ikt3.py \
    --checkpoint experiments/EXPERIMENT_ID/best_model.pt \
    --dataset assist2015 \
    --fold 0 \
    --reference_targets_path data/assist2015/irt_extended_targets_fold0.pkl \
    --batch_size 64
```

**Expected outputs:**
- Training: `metrics_epoch.csv`, `metrics_valid.csv`
- Evaluation: `metrics_test.csv`, `eval_results.json` (with correlations)
- Success if: l_21 < 0.15, l_22 < 0.10, l_23 < 0.15, correlation > 0.85

---

## Pluggable Reference Model Architecture

### Benefits

1. **Scientific Flexibility:**
   - Test multiple theoretical groundings (IRT, BKT, DINA, PFA, etc.)
   - Empirically compare which theory provides best construct validity
   - Enable hybrid approaches (e.g., IRT for difficulty, BKT for learning rates)

2. **Code Reusability:**
   - Single iKT3 model implementation works with all reference models
   - Shared encoder, training loop, evaluation pipeline
   - Only reference-specific logic encapsulated in separate modules

3. **Extensibility:**
   - Add new reference models without modifying iKT3 core
   - Clear interface contract (ReferenceModel ABC)
   - Future models: 2-3 days implementation time

4. **Comparative Analysis:**
   - Direct A/B testing: same architecture, different theoretical constraints
   - Isolates effect of reference model choice
   - Enables meta-analysis: which theories work best for which datasets?

5. **Pedagogical Value:**
   - Demonstrates proper software engineering in ML research
   - Separates concerns: neural architecture vs theoretical grounding
   - Educational tool: shows how different KT theories relate to deep learning

### Future Reference Models (Roadmap)

**Immediate (Phase 1):**
- ‚úÖ **IRT (Rasch)**: 
  - **Parameters**: Œ∏ (student ability), Œ≤ (skill difficulty)
  - **Prediction formula**: M = œÉ(Œ∏ - Œ≤)
  - **Alignment losses**: l_21 (performance), l_22 (difficulty), l_23 (ability)
  
- ‚úÖ **BKT (Bayesian Knowledge Tracing)**:
  - **Parameters per skill**: P(L_0) (prior), P(T) (learns), P(S) (slips), P(G) (guesses)
  - **State**: P(L_t) (mastery probability at time t)
  - **Prediction formula**: P(correct) = P(L_t)√ó(1-P(S)) + (1-P(L_t))√óP(G)
  - **Alignment losses**: l_21 (mastery trajectory), l_22 (parameters if learned)

**Short-term (3-6 months):**

- **DINA** (Deterministic Inputs, Noisy "And" gate):
  - **Parameters**:
    - **Q-matrix**: [num_items √ó num_skills] binary matrix specifying which skills are required for each item
    - **s_j** (slip): Probability of incorrect response when all required skills are mastered
    - **g_j** (guess): Probability of correct response when at least one required skill is not mastered
    - **Œ±_ik**: Binary mastery state (0/1) for student i on skill k
  - **Prediction formula**: 
    - Œ∑_ij = ‚àè_k Œ±_ik^q_jk (conjunctive mastery: student must master ALL skills required by item)
    - P(correct) = (1-s_j)^Œ∑_ij √ó g_j^(1-Œ∑_ij)
  - **Interpretable factors**:
    - Binary skill mastery states (discrete, not continuous)
    - Item-level slip/guess rates
    - Q-matrix structure reveals cognitive model
  - **Alignment losses**:
    - l_21: BCE(mastery_states, Œ±_ref) - binary mastery alignment
    - l_22: MSE(slip_learned, s_ref) + MSE(guess_learned, g_ref)
    - l_23: Q-matrix alignment if learned (optional)
  - **Success thresholds**: l_21 < 0.10 (strict for binary), l_22 < 0.15, corr > 0.85
  
- **PFA** (Performance Factor Analysis):
  - **Parameters**:
    - **Œ≤_k** (difficulty): Difficulty of skill k (similar to IRT but context-dependent)
    - **Œ≥_k** (learning rate): Gain in logit scale per successful practice of skill k
    - **œÅ_k** (decay rate): Penalty in logit scale per failed practice of skill k
    - **m_ik**(successes): Count of successful practices for student i on skill k
    - **n_ik** (failures): Count of failed practices for student i on skill k
  - **Prediction formula**: 
    - logit(P(correct)) = Œ≤_k + Œ≥_k√óm_ik - œÅ_k√ón_ik
    - Incorporates practice history explicitly
  - **Interpretable factors**:
    - Skill difficulty (static)
    - Learning curves via Œ≥_k (positive reinforcement)
    - Forgetting/confusion via œÅ_k (negative reinforcement)
    - Cumulative practice effects (m_ik, n_ik)
  - **Alignment losses**:
    - l_21: BCE(predictions, PFA_ref) - prediction alignment
    - l_22: MSE(Œ≤_learned, Œ≤_ref) - difficulty alignment
    - l_23: MSE(Œ≥_learned, Œ≥_ref) + MSE(œÅ_learned, œÅ_ref) - learning/decay rates
    - l_24: Practice count tracking (optional, for full PFA alignment)
  - **Success thresholds**: l_21 < 0.15, l_22 < 0.10, l_23 < 0.15, corr > 0.85
  - **Note**: Requires tracking success/failure counts per skill per student

**Medium-term (6-12 months):**

- **AFM** (Additive Factor Model):
  - **Parameters**:
    - **Œ≤_k** (intercept): Baseline difficulty for skill k
    - **Œ≥_k** (slope): Learning rate for skill k
    - **Q-matrix**: [num_items √ó num_skills] (can be real-valued, not just binary)
    - **Opportunity count**: Number of practice opportunities per skill
  - **Prediction formula**: 
    - logit(P(correct)) = Œ£_k q_jk √ó (Œ≤_k + Œ≥_k √ó opp_ik)
    - Linear additive effects across multiple skills per item
  - **Interpretable factors**:
    - Multi-skill items: P(correct) depends on combination of skills
    - Skill-specific learning curves
    - Additive assumption: skills contribute independently to performance
  - **Alignment losses**:
    - l_21: BCE(predictions, AFM_ref) - prediction alignment
    - l_22: MSE(Œ≤_learned, Œ≤_ref) - difficulty alignment per skill
    - l_23: MSE(Œ≥_learned, Œ≥_ref) - learning rate alignment per skill
    - l_24: Q-matrix alignment if learned (regularization to cognitive structure)
  - **Success thresholds**: l_21 < 0.15, l_22 < 0.10, l_23 < 0.15, corr > 0.85
  - **Challenge**: Handling multi-skill items requires aggregation mechanism in Head 2
  
- **DAS3H** (Deep Adaptive Skill Strength Simulator for Hierarchical):
  - **Parameters**:
    - **Œ∏_ik(t)**: Time-varying skill strength for student i on skill k at time t
    - **Transfer matrix T**: [num_skills √ó num_skills] capturing prerequisite relationships
    - **Decay rates d_k**: Forgetting rate per skill k
    - **Œ≤_j**: Item difficulty
    - **Learning gain ŒîŒ∏**: Skill strength increase from practice
  - **Prediction formula**: 
    - Œ∏_ik(t) = Œ∏_ik(t-1) √ó e^(-d_k√óŒît) + Œ£_k' T_kk' √ó ŒîŒ∏_k'(success/failure)
    - P(correct) = œÉ(Œ∏_ik(t) - Œ≤_j)
    - Combines IRT-like prediction with temporal dynamics and transfer
  - **Interpretable factors**:
    - Temporal skill decay (forgetting curves)
    - Cross-skill transfer (e.g., algebra helps geometry)
    - Hierarchical skill structure (prerequisites)
    - Time-sensitive predictions (spacing effects)
  - **Alignment losses**:
    - l_21: BCE(predictions, DAS3H_ref) - prediction alignment
    - l_22: MSE(Œ≤_learned, Œ≤_ref) - difficulty alignment
    - l_23: MSE(Œ∏_learned(t), Œ∏_ref(t)) - skill strength trajectories
    - l_24: MSE(T_learned, T_ref) - transfer matrix (captures cognitive structure)
    - l_25: MSE(d_learned, d_ref) - decay rates
  - **Success thresholds**: l_21 < 0.15, l_22 < 0.10, l_23 < 0.20 (looser for temporal), others < 0.15, corr > 0.85
  - **Challenge**: Requires temporal modeling in encoder; most complex reference model

**Comparison of Parameter Complexity:**

| Reference Model | Static Parameters | Dynamic Parameters | Alignment Losses | Implementation Complexity |
|-----------------|-------------------|-------------------|------------------|---------------------------|
| **IRT** | Œ≤ (difficulty) | Œ∏ (ability) | 3 (l_21, l_22, l_23) | Low |
| **BKT** | prior, learns, slips, guesses | P(L_t) | 2 (l_21, l_22 opt) | Low-Medium |
| **DINA** | Q-matrix, slip, guess | Œ± (binary mastery) | 3 (l_21, l_22, l_23 opt) | Medium |
| **PFA** | Œ≤, Œ≥, œÅ | m (successes), n (failures) | 4 (l_21, l_22, l_23, l_24 opt) | Medium |
| **AFM** | Œ≤, Œ≥, Q-matrix | opp (opportunities) | 4 (l_21, l_22, l_23, l_24 opt) | Medium-High |
| **DAS3H** | Œ≤, T, d | Œ∏(t) with decay/transfer | 5 (l_21-l_25) | High |

**Research Extensions:**
- **Hybrid Models**: 
  - Combine IRT difficulty (Œ≤) + BKT learning dynamics (P(T), P(S), P(G))
  - Use DINA Q-matrix with continuous skill strengths instead of binary states
  - Parameters: Œ∏, Œ≤, Q-matrix, P(T), P(S), P(G)
  
- **Multi-Grain Models**: 
  - IRT at item level, BKT at skill level, DAS3H for long-term retention
  - Different reference models at different time scales
  - Parameters: hierarchical with level-specific constraints
  
- **Student-Adaptive**: 
  - Switch reference model based on learner profile (e.g., DINA for beginners, IRT for advanced)
  - Gating mechanism to select reference model per student
  - Parameters: all reference models + gating weights

### Adding New Reference Models

Each new reference model requires:
1. Implement `ReferenceModel` subclass (150-300 lines) - follow IRT example
2. Create target generation script (adapt `compute_irt_extended_targets.py`)
3. Add to `REFERENCE_MODELS` registry in `__init__.py`
4. Update parameter_default.json with model-specific thresholds
5. Test on standard datasets (2-3 experiments)

**Total effort per new model:** 2-3 days

**Example Implementation Template:**

```python
# pykt/reference_models/new_model_reference.py
class NewModelReferenceModel(ReferenceModel):
    def __init__(self, num_skills: int):
        super().__init__("NewModel", num_skills)
    
    def load_targets(self, targets_path: str) -> Dict[str, torch.Tensor]:
        # Load pre-computed targets
        pass
    
    def compute_alignment_losses(self, model_outputs, targets, lambda_weights):
        # Compute model-specific losses
        return {
            'l_21_<name>': ...,
            'l_22_<name>': ...,
            'l_align_total': ...
        }
    
    def get_loss_names(self) -> List[str]:
        return ['l_21_<name>', 'l_22_<name>']
    
    def get_interpretable_factors(self, model_outputs):
        return {'factor1': ..., 'factor2': ...}
```

**Registration:**
```python
# pykt/reference_models/__init__.py
from .new_model_reference import NewModelReferenceModel

REFERENCE_MODELS = {
    'irt': IRTReferenceModel,
    'new_model': NewModelReferenceModel
}
```

---

## Implementation Plan: Future Reference Models

### Design Principle: Pluggable Reference Models (‚úÖ IMPLEMENTED)

The architecture has been successfully implemented with full pluggability support. Adding new reference models only requires implementing the abstract interface‚Äîno changes to core iKT3 code.

**Implemented Components:**

1. **Abstract Reference Model Interface:** ‚úÖ `pykt/reference_models/base.py`
   ```python
   class ReferenceModel(ABC):
       def __init__(self, model_name: str, num_skills: int)
       
       @abstractmethod
       def load_targets(self, targets_path: str) -> Dict[str, torch.Tensor]
       
       @abstractmethod
       def compute_alignment_losses(
           self, model_outputs, targets, lambda_weights
       ) -> Dict[str, torch.Tensor]
       
       @abstractmethod
       def get_loss_names(self) -> List[str]
       
       @abstractmethod
       def get_interpretable_factors(
           self, model_outputs
       ) -> Dict[str, torch.Tensor]
   ```

2. **Model-Specific Implementations:**
   - ‚úÖ `IRTReferenceModel`: Fully implemented with l_21, l_22, l_23
   - ‚è≥ `BKTReferenceModel`: Planned (3-6 months)
   - ‚è≥ `DINAReferenceModel`: Planned (6-12 months)
   - ‚è≥ `PFAReferenceModel`: Planned (6-12 months)

3. **Unified iKT3 Architecture:** ‚úÖ Fully implemented
   - Reference model type specified at initialization
   - Dependency injection pattern for reference model
   - Loss computation fully delegated to reference model
   - No conditional logic in core training loop

**Verified Benefits:**
- ‚úÖ **Extensibility**: Can add BKT without touching iKT3 code
- ‚úÖ **Comparison**: Can A/B test IRT vs future models with same checkpoints
- ‚úÖ **Maintainability**: Each reference model is self-contained module
- ‚úÖ **Scientific Rigor**: Clean separation validated in practice

---

### Phase 1: Data Preparation (‚úÖ COMPLETE for IRT, ‚è≥ Pending for BKT)

**Goal:** Create standardized target files for multiple reference models with unified interface

**Files to Create:**

1. **`pykt/reference_models/base.py`** - Abstract base class
   ```python
   from abc import ABC, abstractmethod
   
   class ReferenceModel(ABC):
       """Abstract interface for theoretical reference models"""
       
       def __init__(self, model_name: str, num_skills: int):
           self.model_name = model_name
           self.num_skills = num_skills
       
       @abstractmethod
       def load_targets(self, targets_path: str) -> dict:
           """Load pre-computed reference targets"""
           pass
       
       @abstractmethod
       def compute_alignment_losses(self, model_outputs: dict, 
                                     targets: dict, 
                                     lambda_weights: dict) -> dict:
           """Compute alignment losses for this reference model"""
           pass
       
       @abstractmethod
       def get_loss_names(self) -> list:
           """Return list of loss component names"""
           pass
       
       @abstractmethod
       def get_interpretable_factors(self, model_outputs: dict) -> dict:
           """Extract interpretable factors for validation"""
           pass
   ```

2. **`pykt/reference_models/irt_reference.py`** - IRT implementation
   ```python
   class IRTReferenceModel(ReferenceModel):
       """Rasch IRT as reference model for construct validity"""
       
       def __init__(self, num_skills: int):
           super().__init__("IRT", num_skills)
       
       def load_targets(self, targets_path: str) -> dict:
           """Load IRT targets with Œ∏_IRT, Œ≤_IRT, M_ref"""
           with open(targets_path, 'rb') as f:
               data = pickle.load(f)
           return {
               'beta_irt': torch.tensor([data['skill_difficulties'][k] 
                                         for k in range(self.num_skills)]),
               'theta_irt': data['student_abilities'],  # dict {uid: Œ∏}
               'm_ref': data['reference_predictions']    # dict {uid: tensor}
           }
       
       def compute_alignment_losses(self, model_outputs, targets, lambda_weights):
           """Compute l_21 (performance), l_22 (difficulty), l_23 (ability)"""
           l_21 = F.binary_cross_entropy(
               model_outputs['mastery_irt'], 
               targets['m_ref']
           )
           l_22 = F.mse_loss(
               model_outputs['beta_learned'], 
               targets['beta_irt']
           )
           l_23 = F.mse_loss(
               model_outputs['theta_learned'], 
               targets['theta_irt']
           )
           return {
               'l_21_performance': l_21,
               'l_22_difficulty': l_22,
               'l_23_ability': l_23,
               'l_align_total': l_21 + l_23  # Combined for Œª weighting
           }
       
       def get_loss_names(self):
           return ['l_21_performance', 'l_22_difficulty', 'l_23_ability']
       
       def get_interpretable_factors(self, model_outputs):
           return {
               'theta': model_outputs['theta_t'],
               'beta': model_outputs['beta_k'],
               'mastery': model_outputs['mastery_irt']
           }
   ```

3. **`pykt/reference_models/bkt_reference.py`** - BKT implementation
   ```python
   class BKTReferenceModel(ReferenceModel):
       """Bayesian Knowledge Tracing as reference model"""
       
       def __init__(self, num_skills: int):
           super().__init__("BKT", num_skills)
       
       def load_targets(self, targets_path: str) -> dict:
           """Load BKT targets with P(L_t), parameters"""
           with open(targets_path, 'rb') as f:
               data = pickle.load(f)
           return {
               'bkt_params': data['bkt_params'],      # {skill_id: {prior, learns, slips, guesses}}
               'bkt_mastery': data['bkt_targets'],    # {uid: P(L_t) trajectories}
               'metadata': data['metadata']
           }
       
       def compute_alignment_losses(self, model_outputs, targets, lambda_weights):
           """
           Compute BKT-specific alignment losses:
           - l_21: Mastery trajectory alignment (MSE with P(L_t))
           - l_22: Parameter regularization (prior, learns, slips, guesses)
           """
           # Mastery alignment
           l_21 = F.mse_loss(
               model_outputs['mastery_bkt'],    # Model's P(L_t) estimate
               targets['bkt_mastery']
           )
           
           # Parameter regularization (if model learns BKT parameters)
           if 'bkt_params_learned' in model_outputs:
               l_22 = self._compute_param_regularization(
                   model_outputs['bkt_params_learned'],
                   targets['bkt_params']
               )
           else:
               l_22 = torch.tensor(0.0)
           
           return {
               'l_21_mastery': l_21,
               'l_22_params': l_22,
               'l_align_total': l_21
           }
       
       def get_loss_names(self):
           return ['l_21_mastery', 'l_22_params']
       
       def get_interpretable_factors(self, model_outputs):
           return {
               'mastery_trajectory': model_outputs['mastery_bkt'],
               'learning_rate': model_outputs.get('learns', None),
               'slip_prob': model_outputs.get('slips', None),
               'guess_prob': model_outputs.get('guesses', None)
           }
   ```

4. **`pykt/reference_models/__init__.py`**
   ```python
   from .base import ReferenceModel
   from .irt_reference import IRTReferenceModel
   from .bkt_reference import BKTReferenceModel
   
   REFERENCE_MODELS = {
       'irt': IRTReferenceModel,
       'bkt': BKTReferenceModel
   }
   
   def create_reference_model(model_type: str, num_skills: int) -> ReferenceModel:
       """Factory function for reference models"""
       if model_type not in REFERENCE_MODELS:
           raise ValueError(f"Unknown reference model: {model_type}. "
                           f"Available: {list(REFERENCE_MODELS.keys())}")
       return REFERENCE_MODELS[model_type](num_skills)
   ```

5. **`examples/compute_reference_targets.py`** - Unified target generation
   ```python
   """
   Generate reference model targets for iKT3 training.
   Supports multiple reference models: IRT, BKT, etc.
   """
   
   def compute_irt_targets(dataset, fold, output_path):
       """Generate extended IRT targets"""
       # Load existing rasch_test_iter300.pkl
       # Compute Œ∏_IRT via Rasch calibration on train+valid
       # Generate M_ref = œÉ(Œ∏_IRT - Œ≤_IRT) for each interaction
       # Save to rasch_extended_targets_fold{fold}.pkl
       pass
   
   def compute_bkt_targets(dataset, fold, output_path):
       """Generate BKT targets"""
       # Use existing compute_bkt_targets.py logic
       # Ensure format matches BKTReferenceModel.load_targets()
       # Save to bkt_extended_targets_fold{fold}.pkl
       pass
   
   if __name__ == "__main__":
       parser = argparse.ArgumentParser()
       parser.add_argument('--reference_model', choices=['irt', 'bkt'], required=True)
       parser.add_argument('--dataset', required=True)
       parser.add_argument('--fold', type=int, required=True)
       parser.add_argument('--output_path', required=True)
       args = parser.parse_args()
       
       if args.reference_model == 'irt':
           compute_irt_targets(args.dataset, args.fold, args.output_path)
       elif args.reference_model == 'bkt':
           compute_bkt_targets(args.dataset, args.fold, args.output_path)
   ```

**Estimated effort:** 2-3 days
- Design abstract interface
- Implement IRT reference model
- Implement BKT reference model  
- Create unified target generation script
- Test on ASSIST2015 and ASSIST2009

---

### Phase 2: Model Implementation (Reference-Model-Agnostic)

**Files to Create:**

1. **`pykt/models/ikt3.py`** - New model class with pluggable reference models
   - Copy iKT2 architecture (encoder, heads, embeddings)
   - **Key Design Change**: Make model agnostic to reference model type
   - Modify `__init__()`:
     ```python
     def __init__(self, num_c, seq_len, d_model, n_heads, num_encoder_blocks,
                  d_ff, dropout, emb_type, reference_model_type='irt'):
         """
         Args:
             reference_model_type: 'irt', 'bkt', etc. - selects reference model
         """
         super().__init__()
         # ... (same encoder architecture as iKT2)
         
         # Reference model interface (injected via factory)
         self.reference_model = None  # Set via set_reference_model()
         self.reference_model_type = reference_model_type
         
         # Model-specific heads (determined by reference model)
         if reference_model_type == 'irt':
             self._init_irt_heads()
         elif reference_model_type == 'bkt':
             self._init_bkt_heads()
     
     def set_reference_model(self, reference_model: ReferenceModel):
         """Inject reference model dependency"""
         self.reference_model = reference_model
     
     def _init_irt_heads(self):
         """Initialize IRT-specific components"""
         self.ability_encoder = nn.Sequential(...)  # Œ∏ extraction
         self.skill_difficulty_emb = nn.Embedding(self.num_c, 1)  # Œ≤
     
     def _init_bkt_heads(self):
         """Initialize BKT-specific components"""
         self.mastery_encoder = nn.Sequential(...)  # P(L_t) estimation
         # Optionally: learnable BKT parameters (prior, learns, slips, guesses)
     ```
   
   - **Forward pass**: Adapts to reference model type
     ```python
     def forward(self, q, r, qry=None):
         # ... (shared encoder processing)
         
         # Reference-model-specific outputs
         if self.reference_model_type == 'irt':
             return self._forward_irt(h, v, qry)
         elif self.reference_model_type == 'bkt':
             return self._forward_bkt(h, v, qry)
     
     def _forward_irt(self, h, v, qry):
         """IRT-specific forward pass"""
         theta_t = self.ability_encoder(h).squeeze(-1)
         beta_k = self.skill_difficulty_emb(qry).squeeze(-1)
         mastery_irt = torch.sigmoid(theta_t - beta_k)
         return {
             'bce_predictions': ...,
             'mastery_irt': mastery_irt,
             'theta_t': theta_t,
             'beta_k': beta_k,
             'beta_learned': self.skill_difficulty_emb.weight.squeeze(),
             ...
         }
     
     def _forward_bkt(self, h, v, qry):
         """BKT-specific forward pass"""
         mastery_bkt = self.mastery_encoder(h)  # Estimate P(L_t)
         return {
             'bce_predictions': ...,
             'mastery_bkt': mastery_bkt,
             ...
         }
     ```
   
   - **Loss computation**: Delegated to reference model
     ```python
     def compute_loss(self, output, targets, ref_targets, lambda_interp, lambda_reg):
         """
         Generic loss computation via reference model interface.
         
         L = (1-Œª) √ó l_bce + c √ó l_stability + Œª √ó l_align
         
         Args:
             output: model forward() outputs
             targets: [B, L] ground truth responses
             ref_targets: dict from ReferenceModel.load_targets()
             lambda_interp: interpretability weight (Œª)
             lambda_reg: stability regularization weight (c)
         """
         # Performance loss (always present)
         l_bce = F.binary_cross_entropy_with_logits(output['logits'], targets)
         
         # Reference model specific losses
         if self.reference_model is None:
             raise ValueError("Reference model not set. Call set_reference_model() first.")
         
         alignment_losses = self.reference_model.compute_alignment_losses(
             model_outputs=output,
             targets=ref_targets,
             lambda_weights={'lambda_interp': lambda_interp, 'lambda_reg': lambda_reg}
         )
         
         # Extract stability and alignment components
         l_stability = alignment_losses.get('l_22_difficulty', torch.tensor(0.0))  # IRT
         l_stability = alignment_losses.get('l_22_params', l_stability)              # BKT fallback
         l_align_total = alignment_losses['l_align_total']
         
         # Combined loss
         total_loss = (1 - lambda_interp) * l_bce + lambda_reg * l_stability + lambda_interp * l_align_total
         
         return {
             'total_loss': total_loss,
             'l_bce': l_bce,
             **alignment_losses  # Include all reference-specific losses
         }
     ```
   
   - Add `create_model()` factory function

**Estimated effort:** 3-4 days (increased due to abstraction layer)

2. **`examples/train_ikt3.py`** - Training script with reference model support
   - Copy structure from `train_ikt2.py`
   - **Key Addition**: Reference model selection
   - Modify argparse:
     ```python
     parser.add_argument('--reference_model', choices=['irt', 'bkt'], required=True,
                        help='Theoretical reference model for alignment')
     parser.add_argument('--reference_targets_path', required=True,
                        help='Path to reference model targets')
     parser.add_argument('--lambda_target', type=float, required=True,
                        help='Target interpretability weight')
     parser.add_argument('--warmup_epochs', type=int, required=True,
                        help='Epochs to reach lambda_target')
     parser.add_argument('--c_stability_reg', type=float, required=True,
                        help='Stability regularization weight (always active)')
     ```
   
   - **Reference model initialization**:
     ```python
     from pykt.reference_models import create_reference_model
     
     # Create model
     model = iKT3(num_c=num_skills, ..., 
                  reference_model_type=args.reference_model)
     
     # Inject reference model
     ref_model = create_reference_model(args.reference_model, num_skills)
     model.set_reference_model(ref_model)
     
     # Load reference targets
     ref_targets = ref_model.load_targets(args.reference_targets_path)
     ```
   
   - Implement warm-up schedule:
     ```python
     def get_lambda_interp(epoch, lambda_target, warmup_epochs):
         return lambda_target * min(1.0, epoch / warmup_epochs)
     ```
   
   - **Training loop**: Pass reference targets to loss computation
     ```python
     for batch in train_loader:
         outputs = model(q, r, qry)
         
         # Get lambda for current epoch
         lambda_current = get_lambda_interp(epoch, args.lambda_target, args.warmup_epochs)
         
         # Compute loss with reference model
         loss_dict = model.compute_loss(
             output=outputs,
             targets=batch['targets'],
             ref_targets=ref_targets,
             lambda_interp=lambda_current,
             lambda_reg=args.c_stability_reg
         )
     ```
   
   - **Metrics logging**: Track all reference-specific losses dynamically
     ```python
     # Get loss names from reference model
     loss_names = ['total_loss', 'l_bce'] + model.reference_model.get_loss_names()
     
     # Log all losses
     for loss_name in loss_names:
         if loss_name in loss_dict:
             writer.add_scalar(f'train/{loss_name}', loss_dict[loss_name], epoch)
     ```

**Estimated effort:** 3-4 days (increased due to reference model integration)

3. **`examples/eval_ikt3.py`** - Evaluation script with reference model support
   - Copy from `eval_ikt2.py`
   - Adapt for reference model interface
   - **Load reference model**:
     ```python
     ref_model = create_reference_model(config['reference_model'], num_skills)
     model.set_reference_model(ref_model)
     ref_targets = ref_model.load_targets(config['reference_targets_path'])
     ```
   
   - Compute validation metrics dynamically:
     ```python
     # Reference-specific metrics
     loss_names = model.reference_model.get_loss_names()
     
     # Compute all losses
     loss_dict = model.compute_loss(outputs, targets, ref_targets, 
                                     lambda_interp=1.0, lambda_reg=config['c_stability_reg'])
     
     # Extract interpretable factors for validation
     factors = model.reference_model.get_interpretable_factors(outputs)
     
     # Compute correlations (IRT: theta-ability, beta-difficulty; BKT: mastery trajectory)
     if config['reference_model'] == 'irt':
         corr_theta = compute_correlation(factors['theta'], ref_targets['theta_irt'])
         corr_beta = compute_correlation(factors['beta'], ref_targets['beta_irt'])
     elif config['reference_model'] == 'bkt':
         corr_mastery = compute_correlation(factors['mastery_trajectory'], 
                                            ref_targets['bkt_mastery'])
     ```
   
   - **Success criterion** (reference-model-specific):
     ```python
     if config['reference_model'] == 'irt':
         success = (loss_dict['l_22_difficulty'] < 0.10 and 
                   loss_dict['l_23_ability'] < 0.15 and 
                   loss_dict['l_21_performance'] < 0.15 and 
                   val_heads_corr > 0.85)
     elif config['reference_model'] == 'bkt':
         success = (loss_dict['l_21_mastery'] < 0.10 and 
                   val_heads_corr > 0.85)
     ```

**Estimated effort:** 2-3 days

---

### Phase 3: Integration with Reproducibility Framework

**Files to Modify:**

1. **`configs/parameter_default.json`**
   - Add new section for iKT3 parameters (reference-model-aware):
     ```json
     {
       "ikt3": {
         "reference_model": "irt",
         "lambda_target": 0.5,
         "warmup_epochs": 50,
         "c_stability_reg": 0.01,
         "reference_targets_path": "data/assist2015/rasch_extended_targets_fold0.pkl"
       },
       "ikt3_irt": {
         "reference_model": "irt",
         "lambda_target": 0.5,
         "warmup_epochs": 50,
         "c_stability_reg": 0.01,
         "reference_targets_path": "data/assist2015/rasch_extended_targets_fold0.pkl",
         "irt_success_thresholds": {
           "l_21_performance": 0.15,
           "l_22_difficulty": 0.10,
           "l_23_ability": 0.15,
           "val_heads_corr": 0.85
         }
       },
       "ikt3_bkt": {
         "reference_model": "bkt",
         "lambda_target": 0.5,
         "warmup_epochs": 50,
         "c_stability_reg": 0.01,
         "reference_targets_path": "data/assist2015/bkt_extended_targets_fold0.pkl",
         "bkt_success_thresholds": {
           "l_21_mastery": 0.10,
           "val_heads_corr": 0.85
         }
       }
     }
     ```
   - Keep existing iKT2 parameters unchanged

2. **`examples/run_repro_experiment.py`**
   - Add model type detection for iKT3
   - **Reference model selection logic**:
     ```python
     if args.model == 'ikt3':
         # Determine reference model from args or defaults
         ref_model = args.reference_model or defaults['ikt3']['reference_model']
         
         # Load reference-specific defaults
         ref_defaults_key = f'ikt3_{ref_model}'
         if ref_defaults_key in defaults:
             model_defaults = defaults[ref_defaults_key]
         else:
             model_defaults = defaults['ikt3']
         
         # Generate command with reference model params
         cmd_params.extend([
             f"--reference_model {ref_model}",
             f"--reference_targets_path {model_defaults['reference_targets_path']}",
             ...
         ])
     ```
   - Generate explicit commands with iKT3-specific parameters
   - Update MD5 hash computation to include iKT3 defaults

3. **`examples/experiment_utils.py`**
   - Add `load_reference_targets()` utility function:
     ```python
     def load_reference_targets(reference_model_type: str, targets_path: str):
         """Load reference model targets via factory"""
         from pykt.reference_models import create_reference_model
         
         # Get num_skills from path or dataset config
         num_skills = extract_num_skills_from_path(targets_path)
         
         # Create reference model and load targets
         ref_model = create_reference_model(reference_model_type, num_skills)
         return ref_model.load_targets(targets_path)
     ```
   - Ensure compatibility with existing metric computation

**Estimated effort:** 1-2 days (increased for reference model variants)

---

### Phase 4: Documentation

**Files to Create/Modify:**

1. **`paper/ikt3_architecture.md`** - Architecture documentation
   - Copy structure from ikt2_asis.md
   - Document pluggable reference model architecture
   - Explain reference model interface (base class and implementations)
   - Document loss formulation with Œª warm-up schedule
   - Compare with iKT2 approach
   - **Sections**:
     - Overview of reference model design pattern
     - IRT reference model specifics (Œ∏, Œ≤, M_ref)
     - BKT reference model specifics (P(L_t), parameters)
     - How to add new reference models (extension guide)

2. **`paper/ikt3_validation.md`** - Validation protocol
   - Document H1, H2, H3 hypotheses (reference-model-agnostic)
   - Explain success criteria for each reference model:
     - IRT: l_21 < 0.15, l_22 < 0.10, l_23 < 0.15, corr > 0.85
     - BKT: l_21 < 0.10, corr > 0.85
   - Describe Pareto analysis procedure
   - Comparison methodology: IRT-aligned vs BKT-aligned iKT3

3. **`paper/reference_models.md`** - Reference model guide
   - How to implement new reference models
   - Interface requirements (ReferenceModel ABC)
   - Examples: IRT and BKT implementations
   - Target file format specifications
   - Loss computation guidelines

4. **`assistant/contribute.txt`** - Update model list
   - Add iKT3 to model registry
   - Document reference model variants (iKT3-IRT, iKT3-BKT)

**Estimated effort:** 1-2 days (increased for reference model documentation)

---

### Phase 5: Testing and Validation

**Tasks:**

1. **Unit Tests:**
   - Test iKT3 loss computation with synthetic data
   - Verify warm-up schedule implementation
   - Check gradient flow through all losses

2. **Integration Tests:**
   - Run 2-epoch training on ASSIST2015
   - Verify metrics logging (5 losses tracked)
   - Check experiment folder structure

3. **Reproducibility Tests:**
   - Launch experiment with run_repro_experiment.py
   - Reproduce from experiment ID
   - Verify config.json integrity (MD5)

4. **Baseline Comparison:**
   - Train iKT3 with Œª_target = 0.5 for 50 epochs
   - Compare with iKT2 baseline on same fold
   - Validate that l_21, l_22, l_23 converge to thresholds

**Estimated effort:** 2-3 days

---

### Phase 6: Pareto Analysis (Scientific Validation)

**Tasks:**

1. **Lambda Sweep (Per Reference Model):**
   - **iKT3-IRT**: Run 11 experiments with Œª_target ‚àà {0.0, 0.1, ..., 1.0}
     - Record: val_auc, l_21, l_22, l_23, val_heads_corr
     - Generate Pareto curve: val_auc vs mean_alignment_loss
   
   - **iKT3-BKT**: Run 11 experiments with Œª_target ‚àà {0.0, 0.1, ..., 1.0}
     - Record: val_auc, l_21_mastery, val_heads_corr
     - Generate Pareto curve: val_auc vs l_21_mastery

2. **Cross-Model Comparison Study:**
   - **Baseline Comparison**: iKT2 (internal alignment) vs iKT3-IRT (external alignment)
     - Test hypothesis: External IRT alignment improves construct validity
     - Compare: Rasch correlation, factor correlations, interpretability scores
   
   - **Reference Model Comparison**: iKT3-IRT vs iKT3-BKT
     - Test hypothesis: Different theoretical groundings lead to different trade-offs
     - Compare Pareto frontiers: which reference model achieves better performance-interpretability balance?
     - Analyze: Do IRT and BKT alignments converge to similar mastery estimates?
   
   - **Three-way Comparison**: iKT2 vs iKT3-IRT vs iKT3-BKT
     - Plot overlaid Pareto curves
     - Identify optimal Œª for each variant
     - Document trade-off characteristics

3. **Statistical Validation (Reference-Model-Specific):**
   
   **For iKT3-IRT:**
   - Compute correlations:
     - corr(Œ∏_learned, Œ∏_IRT) - ability alignment
     - corr(Œ≤_learned, Œ≤_IRT) - difficulty alignment
     - corr(M_IRT, M_ref) - prediction alignment
   - Verify construct validity: convergent alignment with psychometric calibration
   - Compare with iKT2's IRT correlations (Kendall œÑ, Spearman œÅ)
   
   **For iKT3-BKT:**
   - Compute correlations:
     - corr(mastery_learned, P(L_t)_BKT) - mastery trajectory alignment
     - Per-skill learning rate agreement (if applicable)
   - Verify temporal consistency: do learned trajectories follow BKT dynamics?
   - Compare with classical BKT forward inference
   
   **Cross-Validation:**
   - Test if iKT3-IRT's mastery estimates correlate with BKT's P(L_t)
   - Test if iKT3-BKT's mastery estimates correlate with IRT's M_ref
   - Hypothesis: Both should agree on high/low mastery states despite different formulations

4. **Qualitative Analysis:**
   - **Case Studies**: Select 5-10 students and visualize:
     - IRT: Œ∏_learned trajectory, Œ≤_learned values, M_IRT predictions
     - BKT: P(L_t) trajectories, learning rate effects
   - **Error Analysis**: When do models disagree? Which reference model is more robust?
   - **Interpretability Assessment**: Which model provides clearer pedagogical insights?

5. **Document Findings:**
   - Paper sections:
     - Experimental setup (datasets, hyperparameters, reference models)
     - Pareto analysis results (curves, optimal Œª values)
     - Statistical validation (correlations, significance tests)
     - Qualitative findings (case studies, error patterns)
     - Discussion: Practical recommendations for reference model selection

**Estimated effort:** 5-7 days (increased for multi-reference comparison)

---

## Training and Validation Protocol

### Training Approach (Single-Phase with Warm-Up)

Train with difficulty regularization always active and interpretability alignment ramping up via Œª warm-up:

```
L(epoch) = (1 - Œª(epoch)) √ó l_bce + c √ó l_22 + Œª(epoch) √ó (l_21 + l_23)

where:
  Œª(epoch) = Œª_target √ó min(1, epoch / warmup_epochs)
  
Parameters:
  - Œª_target: Target interpretability weight (e.g., 0.5)
  - warmup_epochs: Number of epochs to reach Œª_target (e.g., 50)
  - c: Fixed constant = 0.01 (always active)
```

**Rationale for Single-Phase:**
- **l_22 (difficulty regularization)**: Always active from epoch 0 to maintain Œ≤ stability
- **l_21, l_23 (interpretability alignment)**: Gradually introduced via Œª warm-up
- No need for sequential phases since difficulty regularization serves a different purpose (stability vs interpretability)
- Simpler implementation and easier to analyze for Pareto curves

**Warm-Up Benefits:**
- **Early epochs (Œª ‚âà 0)**: 
  - Model focuses on predictive performance (l_bce)
  - Difficulty values stay anchored to IRT calibration (c√ól_22)
- **Middle epochs**: 
  - Gradually introduces ability and performance alignment (l_21, l_23)
- **Late epochs (Œª = Œª_target)**: 
  - Full balance between performance and interpretability
  - Difficulty regularization continues to prevent drift

### Validation Protocol

**Per-Epoch Validation:**
1. **Compute all metrics** on validation set:
   - l_22, l_23, l_21 (alignment losses)
   - val_heads_corr (Pearson correlation between heads)
   - val_auc, val_acc (performance metrics)

2. **Track convergence**:
   - Monitor l_22, l_23, l_21 trajectories
   - Check if thresholds are approached
   - Observe performance-interpretability trade-off

**Final Validation (After Training):**
1. **Check success criterion**:
   ```
   ‚úÖ Valid IRT Constructs ‚ü∫ (l_22 < 0.10) ‚àß (l_23 < 0.15) ‚àß (l_21 < 0.15) ‚àß (val_heads_corr > 0.85)
   ```

2. **If criterion fails**:
   - Increase Œª_target and retrain
   - Check for implementation bugs if l_21 high but l_22/l_23 low
   - Consider architectural modifications if val_heads_corr low despite low alignment losses

### Pareto Analysis (Multiple Runs)

To trace the performance-interpretability trade-off curve:

1. **Run experiments with different Œª_target values**:
   - Œª_target ‚àà {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}
   - Keep all other hyperparameters fixed (c = 0.01, warmup_epochs = 50)
   - Note: Even at Œª_target = 0.0, l_22 remains active for difficulty stability

2. **For each run, record**:
   - Performance: val_auc, val_acc
   - Interpretability: l_22, l_23, l_21, val_heads_corr
   
3. **Plot Pareto curve**:
   - X-axis: Mean alignment loss = (l_22 + l_23 + l_21) / 3
   - Y-axis: val_auc
   - Each point represents one Œª_target value

4. **Identify optimal Œª_target**:
   - Sweet spot: Highest val_auc while meeting validity thresholds
   - Report as: "Model achieves X% AUC with valid IRT constructs (Œª = Y)"

**Scientific Justification**:
- **H1 (l_22, l_23)**: Establishes convergent validity‚Äîlearned factors align with independent psychometric calibration
- **H2 (l_21)**: Establishes predictive validity‚Äîthe IRT mechanism is correctly implemented
- **H3 (val_heads_corr)**: Establishes integration‚Äîaccuracy and interpretability are compatible

**Why This is Sufficient**:
1. **Construct Validity**: H1 proves the factors are psychometrically grounded
2. **Functional Validity**: H2 proves the IRT formula works correctly with learned factors
3. **Architectural Validity**: H3 proves the dual-head design achieves its goals

Additional hypotheses (learning progression, factor independence) are **theoretical expectations** that provide additional confidence but are not strictly necessary for construct validation. The minimal set directly tests whether Head 2 implements valid IRT constructs through actionable, optimizable losses.

---

## Diagnostic Protocol: The Alignment Paradox

### Problem Statement

During iKT3 experiments, we observed paradoxical patterns where alignment metrics appear good but performance suffers, or vice versa. This protocol helps diagnose what's actually happening.

### Paradox Type 1: Good Individual Alignment, Poor Combined Prediction (Historical)

**Early Experiment Results (30 epochs, Œª = 0.30, static IRT):**
- ‚úÖ l_22_difficulty = 0.005 (threshold: 0.10) - Excellent Œ≤ alignment
- ‚úÖ l_23_ability = 0.018 (threshold: 0.15) - Excellent Œ∏ alignment  
- ‚úó l_21_performance = 9.58 (threshold: 0.15) - **64x over threshold**
- ‚úó Mastery prediction correlation = 0.31 (threshold: 0.85) - Poor

**The Paradox:** How can Œ∏ and Œ≤ individually match IRT values almost perfectly (low MSE), yet their combination through the IRT formula `M_IRT = œÉ(Œ∏ - Œ≤)` produces predictions that disagree fundamentally with the reference model?

**Root Cause (Resolved):**
- Insufficient Œª warm-up (stopped at 60% of target)
- Weak Œ≤ regularization (c = 0.01 too low)
- Static IRT targets allowing scale drift

### Paradox Type 2: Poor Alignment Despite Training (Current - Dec 7, 2025)

**Recent Experiment Results (Exp 686759, 30 epochs, Œª_target=0.5, dynamic IRT):**
- ‚úÖ Scale collapse SOLVED: Œ∏_std = 4.03 (vs 0.14 baseline, +2806%)
- ‚úÖ Individual differences learned: Œ∏ range = 29.6 units
- ‚úÖ Decent prediction: Test AUC = 0.7202 (model learns to predict)
- ‚úó **l_21_performance = 4.058 (threshold < 0.15, FAIL 27√ó over)**
- ‚úó **l_22_difficulty = 0.144 (threshold < 0.10, FAIL 1.4√ó over)**
- ‚úó **l_23_ability = 6.792 (threshold < 0.15, FAIL 45√ó over)**

**The NEW Paradox:** Model learns predictive features (AUC=0.72) but completely fails to learn IRT-consistent parameters. ALL three alignment losses far exceed thresholds despite having them in the training objective with Œª=0.15.

**Critical Evidence - Baseline Comparison:**

| Metric | Dynamic (Œª‚âà0.15) | Baseline (Œª‚âà0.07) | Analysis |
|--------|------------------|-------------------|----------|
| l_21 | 4.058 | 4.574 | Both FAIL (>27√ó threshold) |
| l_22 | 0.144 | 0.033 | Dynamic 4.4√ó worse |
| l_23 | 6.792 | 0.223 | Different targets (dynamic vs static) |
| AUC | 0.7202 | 0.7182 | Minimal difference (+0.29%) |

**Key Insight:** Even baseline with LOWER Œª=0.07 has l_21=4.57. This proves the problem is NOT lambda being too high - it's that **IRT alignment and prediction performance are fundamentally incompatible objectives for this dataset**.

**Root Cause Analysis:**

1. **IRT formula doesn't fit the data:**
   - l_21 measures BCE(œÉ(Œ∏ - Œ≤), M_ref) 
   - High l_21 = 4.06 means learned IRT formula predictions don't match M_ref
   - This occurs with BOTH Œª=0.15 and Œª=0.07, suggesting structural issue
   - Model finds better predictive patterns than IRT's simple Œ∏ - Œ≤ subtraction

2. **M_ref targets may be unreliable:**
   - M_ref computed from external IRT calibration on training data
   - If IRT assumptions don't hold (constant ability, unidimensional skills, no learning), M_ref is noisy/wrong
   - Model correctly ignores bad targets in favor of actual response patterns

3. **Œ≤ learning depends on IRT compatibility:**
   - With Œª=0.07 (baseline): l_22 = 0.033 (good Œ≤ alignment)
   - With Œª=0.15 (dynamic): l_22 = 0.144 (poor Œ≤ alignment)
   - Higher Œª forces model to satisfy incompatible IRT formula, breaking Œ≤ learning
   - Model must distort Œ≤ to make œÉ(Œ∏ - Œ≤) match M_ref, which conflicts with true difficulties

4. **Dynamic Œ∏ targets exacerbate incompatibility:**
   - l_23 = 6.79 (30√ó higher than static baseline)
   - Matching time-varying trajectories is harder
   - But if trajectories themselves are wrong (IRT assumes constant Œ∏), harder task = worse outcome

### Root Cause Analysis

The paradox arises from **uncoordinated factor learning** where:

1. **Low MSE ‚â† Correct Combination**
   - MSE measures point-wise distance: `||Œ∏_learned - Œ∏_IRT||¬≤`
   - But IRT predictions depend on **subtraction**: `Œ∏ - Œ≤`
   - Small individual errors compound during subtraction if they have opposite signs
   - Example: If Œ∏_learned = Œ∏_IRT + 0.1 and Œ≤_learned = Œ≤_IRT - 0.1, then:
     - l_22 = l_23 ‚âà 0.01 (excellent)
     - But (Œ∏_learned - Œ≤_learned) = (Œ∏_IRT - Œ≤_IRT) + 0.2 (20% error)

2. **Sigmoid Amplification**
   - The sigmoid function œÉ(x) is highly sensitive around x = 0 (decision boundary)
   - Small changes in (Œ∏ - Œ≤) near 0 cause large changes in predictions
   - BCE loss exponentially penalizes these prediction differences
   - This explains why l_21 can be orders of magnitude higher than l_22 + l_23

3. **Insufficient Œª Weight**
   - Training stopped at epoch 30, Œª only reached 0.30 (60% of warm-up)
   - l_21 contributed only 0.30 √ó 9.58 = 2.87 to total loss
   - l_bce contributed much more with stronger gradient signal
   - Model prioritized performance over IRT alignment

4. **Weak Œ≤ Regularization**
   - c = 0.01 is very small, contributing only 0.01 √ó 0.005 = 0.00005 to loss
   - Even with low l_22, Œ≤ embeddings can drift in ways that hurt coordination
   - Need stronger anchoring to prevent scale/offset mismatch

### Correlation as Diagnostic Tool

To diagnose whether high l_21 is due to:
- **Scale/offset mismatch** (fixable with more training), or
- **Fundamental disagreement** (requires architectural investigation)

We compute the **Pearson correlation** between model and reference predictions:

```python
from scipy.stats import pearsonr

# Flatten predictions, remove padding/NaN
mastery_irt_clean = model_outputs['mastery_irt'].flatten()[valid_mask]
mastery_ref_clean = ref_targets['m_ref'].flatten()[valid_mask]

# Compute correlation
correlation, _ = pearsonr(mastery_irt_clean, mastery_ref_clean)
```

**Interpretation:**

| Correlation | BCE Loss | Diagnosis | Prognosis |
|-------------|----------|-----------|-----------|
| > 0.85 | High | Scale/offset mismatch | ‚úÖ Fixable with more training |
| 0.5 - 0.85 | High | Partial alignment | ‚ö†Ô∏è Needs longer training + stronger c |
| < 0.5 | High | Fundamental disagreement | ‚úó Architectural issue |

**Experiment 290365 Results:**
- Pearson correlation: **0.31** (poor)
- Spearman correlation: **0.49** (moderate rank preservation)
- Diagnosis: **Fundamental disagreement** - not a simple scaling issue

### Why Low Correlation Indicates Architectural Problems

**High correlation + High BCE:**
```
Model:     [0.2, 0.4, 0.6, 0.8]
Reference: [0.3, 0.5, 0.7, 0.9]
Correlation: 1.0 (perfect ordering)
BCE: High (consistent 0.1 offset)
```
‚Üí Model understands relative difficulty/ability correctly, just needs calibration

**Low correlation + High BCE:**
```
Model:     [0.2, 0.8, 0.3, 0.7]
Reference: [0.9, 0.3, 0.7, 0.4]
Correlation: Low (opposite orderings)
BCE: High (fundamentally wrong)
```
‚Üí Model learned different patterns than IRT expects - lack of construct validity

### Recommended Actions (Revised - Dec 7, 2025)

#### STRATEGIC CHOICE REQUIRED: Performance vs Interpretability

**The Fundamental Dilemma:**
Evidence shows IRT alignment and prediction performance are **incompatible** for this dataset:
- Baseline (Œª=0.07): Good Œ≤ (l_22=0.033), poor predictions (l_21=4.57), low AUC (0.718)
- Dynamic (Œª=0.15): Worse Œ≤ (l_22=0.144), poor predictions (l_21=4.06), marginal AUC gain (0.720)
- Both configurations: Model learns predictive features that disagree with IRT

**Choose ONE path:**

---

#### Path A: Performance-First (Abandon IRT Alignment)

**Philosophy:** Prioritize prediction accuracy, accept that learned factors won't match IRT scale

**Action:** Reduce Œª_target from 0.5 to 0.0 (remove alignment entirely)

**Rationale:**
- IRT formula doesn't fit this dataset (l_21=4+ regardless of Œª)
- M_ref targets are unreliable (computed from incompatible IRT model)
- Forcing alignment breaks Œ≤ learning and hurts performance
- Better to learn predictive Œ∏/Œ≤ even if not IRT-scale

**Expected Outcome:**
- ‚úÖ Test AUC: 0.720 ‚Üí ‚â•0.73 (competitive with simpleKT)
- ‚úÖ l_22: 0.144 ‚Üí <0.05 (proper Œ≤ learning restored)
- ‚ùå l_21, l_23: Will increase (no longer optimized)
- ‚ö†Ô∏è Interpretability: Œ∏, Œ≤ are predictive features, NOT IRT-scale parameters

**Trade-off:** High performance, low theory-grounding

**Recommendation:** Choose this if paper focuses on **prediction accuracy** and interpretability is secondary

---

#### Path B: Alignment-First (Fix IRT Incompatibility)

**Philosophy:** Prioritize IRT consistency, investigate why current approach fails

**Status:** ‚úÖ ROOT CAUSE IDENTIFIED (Dec 8, 2025)

**Investigation Results:**

1. **M_ref Target Quality: ‚ùå POOR**
   ```bash
   python tmp/validate_irt_quick.py
   ```
   **Results:**
   - Pearson correlation: **0.1922** (should be > 0.7)
   - AUC: **0.6274** (should be > 0.75)
   - MAE: **0.3588** (high calibration error)
   
   **Diagnosis:** IRT reference predictions M_ref do NOT correlate with actual student responses. The Rasch model œÉ(Œ∏ - Œ≤) fundamentally does not fit the ASSIST2015 dataset.
   
   **Implication:** l_21 = BCE(M_IRT, M_ref) forces the model to match **bad targets**. Even with perfect alignment, the model would perform poorly because M_ref itself is wrong

**Required Actions to Fix:**

2. **Recalibrate IRT from Scratch:**
   - Current IRT calibration produced correlation=0.19 with ground truth
   - May have convergence issues or incorrect implementation
   - Try longer calibration (more iterations)
   - Verify Newton-Raphson optimization converging
   - Check if skill difficulties Œ≤ are reasonable

3. **Test Rasch Assumptions:**
   - **Unidimensionality**: Single ability dimension may be insufficient
   - **Constant ability**: Students learn during sequences (violates Rasch)
   - **Local independence**: May have dependencies between consecutive items
   - If assumptions violated, Rasch model inappropriate

4. **Try More Flexible IRT Models:**
   - **2PL**: M = œÉ(Œ± √ó (Œ∏ - Œ≤)) with discrimination parameters Œ±
   - **3PL**: Adds guessing parameter
   - **Multidimensional IRT**: Multiple ability dimensions
   - More parameters may better fit data

5. **Switch to Different Reference Model:**
   - **BKT** (Bayesian Knowledge Tracing): Models knowledge state transitions
   - **DAS3H**: Deep knowledge tracing as reference
   - **Teacher model**: Use strong black-box model (e.g., SAINT) as reference
   - May have better predictive validity than IRT

**Expected Outcome if Investigations Succeed:**
- ‚úÖ M_ref correlation > 0.7 (valid reference targets)
- ‚úÖ l_21, l_22, l_23 decrease below thresholds
- ‚úÖ True IRT-scale interpretability
- ‚ùå Test AUC likely 0.65-0.70 (lower than performance-first)

**Expected Outcome if IRT Fundamentally Incompatible:**
- ‚ùå Cannot fix correlation (dataset violates Rasch assumptions)
- ‚ùå Need to abandon IRT and try different model
- ‚Üí Switch to Path A (performance-first) or use non-IRT reference

**Trade-off:** High theory-grounding and interpretability, lower prediction performance

**Recommendation:** Only choose this if paper focuses on **educational theory and interpretability** as primary contribution, and competitive AUC is secondary. Given poor M_ref quality (0.19 correlation), fixing this requires significant investigation and may not be possible with Rasch IRT.

---

#### DEPRECATED: Path C (Phase 1 - Lambda Reduction)

**Original Plan:** Reduce Œª_target from 0.5 to 0.05

**Status:** ‚ö†Ô∏è **TESTED & CONFIRMED DEPRECATED** (Exp 322419, Dec 8, 2025)

**Experimental Results (Œª=0.05 vs Œª=0.5):**

| Metric | Œª=0.05 (Exp 322419) | Œª=0.5 (Exp 686759) | Change |
|--------|---------------------|--------------------:|--------|
| **Best Epoch** | 7 | 15 | Earlier convergence |
| **Actual Œª at best** | 0.007 | 0.15 | 21√ó lower |
| **Test AUC** | 0.7204 | 0.7202 | **+0.02 pp** |
| **l_21 (performance)** | 4.225 | 4.058 | **+4.1% WORSE** |
| **l_22 (difficulty)** | 0.028 | 0.144 | **-80.9% BETTER** ‚úÖ |
| **l_23 (ability)** | 6.929 | 6.792 | **+2.0% WORSE** |

**Key Findings:**
1. **Performance Impact:** Essentially neutral (+0.02 pp, negligible)
2. **Alignment Failures Persist:**
   - l_21: STILL FAILS (4.22 vs 4.06) - IRT incompatibility unchanged
   - l_22: PASSES (0.028 vs 0.144) - Dramatic 81% improvement confirms higher Œª breaks Œ≤ learning
   - l_23: STILL FAILS (6.93 vs 6.79) - Ability alignment unchanged
3. **Critical Insight:** Lower Œª dramatically improves Œ≤ learning (l_22) but **cannot fix l_21 or l_23**

**Why this confirms IRT incompatibility:**
- l_21 remains 28√ó over threshold at Œª=0.007 (same as Œª=0.15)
- IRT formula œÉ(Œ∏ - Œ≤) fundamentally disagrees with dataset patterns
- Lambda tuning only shifts **which component breaks**, not whether alignment succeeds
- Trade-off is not smooth: either Œ≤ learning OR IRT alignment, not both

**Conclusion:**
- ‚ùå Lambda reduction does NOT solve fundamental IRT incompatibility
- ‚úÖ Confirms Œ≤ learning improves with lower Œª (0.028 vs 0.144)
- ‚úÖ Validates hypothesis: performance and alignment are incompatible for this dataset
- üìå Must choose strategic path: Path A (performance, Œª=0.0) or Path B (fix IRT)

#### 2. SHORT-TERM: Separate Lambda Components (Phase 2)

**Problem:** Single Œª for all alignment losses (l_21, l_23) doesn't account for magnitude differences

**Action:** Implement individual lambda weights

**Current Loss:**
```python
L = (1-Œª) √ó l_bce + c √ó l_22 + Œª √ó (l_21 + l_23)
```

**Proposed Loss:**
```python
L = (1-Œª_pred) √ó l_bce + c √ó l_22 + Œª_21 √ó l_21 + Œª_23 √ó l_23
```

**Recommended Weights:**
- Œª_21 = 0.1 (performance alignment - moderate, magnitude ~4)
- c = 0.01 (difficulty regularization - always active)
- Œª_23 = 0.01-0.02 (ability alignment - very low due to high magnitude ~7)

**Rationale:**
- Fine-grained control over each loss component
- Accounts for different loss magnitudes
- Prevents any single loss from dominating
- Better balance between performance and interpretability

**Implementation:** Requires modifying `pykt/reference_models/irt_reference.py` and `examples/train_ikt3.py`

**Status:** Deferred pending Phase 1 results

#### 3. LEGACY: Complete Lambda Warm-Up (Historical Solution)

**Historical Problem (Paradox Type 1):** Training stopped at epoch 30, Œª = 0.30 (60% of target)

**Historical Action:** Train for 60+ epochs to reach Œª = 0.50

**Current Status:** ‚ö†Ô∏è **NOT RECOMMENDED** - Analysis shows even Œª=0.30 was too high given l_23 magnitude. With Œª_target=0.05, 30 epochs reaches Œª=0.03 at epoch 30, which is appropriate.

**Lesson Learned:** Warm-up duration should be matched to loss magnitudes, not set arbitrarily. With dynamic IRT (l_23 ‚âà 7), full warm-up to Œª=0.5 would sacrifice too much performance.

#### 4. OPTIONAL: Increase Difficulty Regularization

**Problem:** c = 0.01 may be too weak for Œ≤ stability

**Action:** Consider increasing c from 0.01 to 0.1 if l_22 remains high after Phase 1

**Status:** HOLD - Wait for Phase 1 results. If Œª reduction fixes Œ≤ learning (l_22 < 0.10), no change needed.

**Rationale:** With lower Œª, more gradient flows to BCE prediction which naturally improves Œ≤ learning. Only increase c if this doesn't happen.

#### 5. MONITORING: Add Correlation Tracking (Recommended)

**Purpose:** Distinguish scale mismatch from fundamental disagreement

**Status:** ‚ö†Ô∏è **NOT CURRENTLY IMPLEMENTED** - Future enhancement

**Implementation:**
In `examples/train_ikt3.py`, add to validation:
```python
from scipy.stats import pearsonr

# During validation
mastery_irt = outputs['mastery_irt'].detach().cpu().numpy().flatten()
mastery_ref = batch_ref_targets['m_ref'].cpu().numpy().flatten()
valid_mask = ~np.isnan(mastery_ref)

corr, _ = pearsonr(mastery_irt[valid_mask], mastery_ref[valid_mask])
metrics_dict['val_mastery_corr'] = corr
```

**Success Criterion:** 
- Correlation > 0.85: Excellent alignment (scale/offset issue only)
- Correlation 0.5-0.85: Partial alignment (needs tuning)
- Correlation < 0.5: Fundamental disagreement (architectural issue)

**Current Status:** With Œª=0.15, achieving good correlation but poor performance suggests we need to prioritize performance over alignment, not increase alignment further.

#### 6. ADVANCED: Remove l_23 Entirely (Radical Alternative)

**If Phase 1 and Phase 2 both fail to achieve AUC ‚â• 0.73:**

Consider removing ability alignment loss completely:

```python
# Simplified loss (no Œ∏ alignment)
L = (1-Œª) √ó l_bce + c √ó l_22 + Œª √ó l_21
```

**Rationale:**
- Œ∏ values still computed for interpretability (not aligned to IRT)
- Œ≤ values anchored to IRT (for theoretical grounding)
- Performance predictions aligned to IRT (for accuracy)
- No forcing Œ∏ to match IRT trajectories (which may be noisy)

**Trade-off:**
- ‚úÖ Better performance (no high-magnitude l_23 interference)
- ‚úÖ Simpler optimization (one less loss component)
- ‚ùå Less theoretically grounded (Œ∏ not guaranteed to match IRT scale)
- ‚ùå Interpretability reduced (Œ∏ values not directly comparable to IRT)

### Summary: Strategic Decision Required (Dec 7, 2025)

**Critical Finding:** IRT alignment and prediction performance are **fundamentally incompatible** for ASSIST2015 dataset. Evidence:

| Evidence | Finding |
|----------|---------|
| l_21 @ Œª=0.07 | 4.574 (27√ó over threshold) |
| l_21 @ Œª=0.15 | 4.058 (27√ó over threshold) |
| Interpretation | IRT formula doesn't fit data regardless of Œª |
| l_22 improvement | 0.033 ‚Üí 0.144 (worse with higher Œª) |
| Conclusion | Higher Œª breaks learning, doesn't fix alignment |

**Two Valid Paths Forward:**

| Aspect | Path A: Performance-First | Path B: Alignment-First |
|--------|---------------------------|-------------------------|
| **Philosophy** | Prediction > Theory | Theory > Prediction |
| **Action** | Set Œª=0.0, remove IRT alignment | Investigate IRT incompatibility |
| **AUC Expected** | ‚â•0.73 (competitive) | 0.65-0.70 (acceptable) |
| **Alignment** | Poor (but irrelevant) | Good (if investigations succeed) |
| **Interpretability** | Features, not IRT-scale | True IRT parameters |
| **Paper Focus** | ML performance | Educational theory |
| **Risk** | No theory grounding | Never competitive performance |

**Recommendation:**
1. **For ML/performance paper:** Choose Path A (Œª=0.0)
2. **For educational theory paper:** Choose Path B (investigate + fix IRT)
3. **Middle ground:** Path A + document learned factors correlate with IRT (even if not same scale)

### Deprecated Approaches (Tested & Rejected)

**Phase 1 (Œª_target=0.05):** ‚úÖ Tested, ‚ùå Does not solve problem
- **Hypothesis:** Œª too high causing alignment failures
- **Test:** Exp 322419 with Œª_target=0.05 (actual Œª=0.007 at best epoch)
- **Results:**
  - AUC: No improvement (+0.02 pp, essentially unchanged)
  - l_21: STILL FAILS (4.225, 28√ó over threshold)
  - l_22: PASSES (0.028, dramatic improvement)
  - l_23: STILL FAILS (6.929, 46√ó over threshold)
- **Conclusion:** IRT incompatibility confirmed - lambda reduction cannot fix
- **Recommendation:** Choose Path A (Œª=0.0) or Path B (fix IRT), not intermediate Œª values

**Phase 2 (Separate Œª components):** ‚ö†Ô∏è Won't solve fundamental issue
- Fine-tuning weights can't fix incompatible objectives
- If IRT formula doesn't fit data, no weight balancing helps
- Only useful after choosing Path B and fixing IRT compatibility
- **Status:** Not tested, not recommended given Phase 1 results

### Comparison Baselines

**Experimental Results:**

| Experiment | Œª_target | Actual Œª | Best Epoch | Test AUC | l_21 | l_22 | l_23 | Interpretation |
|------------|----------|----------|------------|----------|------|------|------|----------------|
| 161656 (static) | 0.5 | 0.07 | 7 | 0.7182 | 4.574 | 0.033 | 0.223 | Baseline: Low Œª, good Œ≤, collapsed Œ∏ |
| 686759 (dynamic) | 0.5 | 0.15 | 15 | 0.7202 | 4.058 | 0.144 | 6.792 | Higher Œª: breaks Œ≤, high l_23 |
| **322419 (Phase 1)** | **0.05** | **0.007** | **7** | **0.7204** | **4.225** | **0.028** | **6.929** | **Lowest Œª: PASSES l_22, FAILS l_21/l_23** |
| simpleKT | N/A | N/A | N/A | 0.7248 | N/A | N/A | N/A | No IRT constraint, pure performance |

**Key Insights:**
1. **AUC essentially constant** (0.7182 ‚Üí 0.7204) across Œª ‚àà [0.007, 0.15] - IRT alignment doesn't improve performance
2. **l_21 always fails** (4.0-4.6) regardless of Œª - IRT formula incompatible with dataset
3. **l_22 inversely related to Œª** - Higher Œª breaks Œ≤ learning (0.028 @ Œª=0.007 ‚Üí 0.144 @ Œª=0.15)
4. **l_23 always fails** (6.8-6.9) - Dynamic IRT trajectories don't match reference regardless of weight
5. **simpleKT superior** - Proves competitive performance possible only by abandoning IRT constraints

### Lessons Learned (Updated Dec 8, 2025)

1. **Dynamic IRT solves scale collapse** - time-varying trajectories force individual differences (Œ∏_std: 0.14 ‚Üí 4.03)

2. **Reference model compatibility is NOT guaranteed** - IRT assumptions may not hold for dataset:
   - l_21 ‚âà 4.0-4.6 across Œª ‚àà [0.007, 0.15] (tested at 3 values)
   - IRT formula œÉ(Œ∏ - Œ≤) fundamentally disagrees with observed patterns
   - Must validate reference model predictions against ground truth BEFORE training
   - **Evidence:** Exp 322419 (Œª=0.007) still has l_21=4.225, proving incompatibility persists even at minimal IRT weight

3. **Lambda tuning cannot fix incompatible objectives:**
   - If reference model is wrong, no Œª value will achieve both performance and alignment
   - Higher Œª (0.15) breaks Œ≤ learning (l_22=0.144) without improving IRT fit
   - Lower Œª (0.007) maintains Œ≤ (l_22=0.028) but still fails IRT alignment (l_21=4.225, l_23=6.929)
   - **Validated by Exp 322419:** Reducing Œª by 21√ó (0.15 ‚Üí 0.007) improves l_22 by 81% but leaves l_21/l_23 unchanged

4. **Trade-off is binary, not smooth:**
   - Can have good Œ≤ learning (l_22 < 0.10) OR high IRT weight (Œª > 0.05), not both
   - AUC remains constant (~0.72) across all Œª values tested - IRT alignment neither helps nor severely hurts prediction
   - Must choose: performance-first (Œª=0.0) or alignment-first (fix IRT + accept lower AUC)

5. **Validation protocol for reference models:**
   - Before implementing alignment losses, validate reference predictions correlate with ground truth
   - Check alignment losses at Œª=0 (forward pass only) to detect incompatibility early
   - If baseline l_21 > 1.0 or l_23 > 1.0, investigate reference model before training
   - Early detection prevents wasted compute on incompatible objectives

6. **Œ≤ regularization works independently:**
   - l_22 responds correctly to Œª changes (0.028 @ Œª=0.007 vs 0.144 @ Œª=0.15)
   - Difficulty regularization is separable from performance/ability alignment
   - Can maintain Œ≤ stability even when abandoning IRT alignment (Path A feasible)

7. **Performance ceiling without IRT:**
   - simpleKT achieves 0.7248 with no IRT constraints
   - iKT3 with IRT alignment stuck at 0.7202-0.7204
   - Gap (~0.004-0.005) suggests IRT constraints slightly harmful, not beneficial
   - To match simpleKT, must remove IRT alignment entirely (Path A)
   - Must choose: optimize for performance OR alignment, not both

4. **Alignment losses reveal model compatibility:**
   - l_21 measures if learned IRT formula matches reference predictions
   - High l_21 (>4) = reference model incompatible with dataset
   - l_22 measures if learned Œ≤ matches reference difficulties
   - l_22 degradation (0.033 ‚Üí 0.144) = higher Œª forces incompatible alignment

5. **Performance-interpretability trade-off may be BINARY:**
   - Not a smooth Pareto curve with tunable Œª
   - Either: competitive performance (AUC ‚â•0.73) with weak/no alignment
   - Or: strong alignment (l_21, l_22, l_23 < thresholds) with poor performance (AUC ~0.65)
   - Middle ground (Œª=0.15) achieves neither

6. **Theory-grounding requires compatible theory:**
   - Can't force IRT consistency if IRT doesn't describe the data
   - Better to learn predictive factors and validate they correlate with theory
   - Than to optimize for alignment with wrong theoretical model

7. **Start with reference model validation, not training:**
   - Before implementing iKT architecture, validate IRT predictions match student responses
   - If M_ref has poor correlation with actual data, fix reference model first
   - Architecture cannot compensate for incompatible theoretical foundation

### Updated Diagnostic Protocol

**When you see paradoxical results, ask these questions IN ORDER:**

#### 1. Is the reference model compatible with the dataset?

**Check:** Does l_21 remain high (>1.0) across multiple Œª values?

**Diagnostic:**
```python
# Compare l_21 across experiments with different Œª
if l_21_at_low_lambda > 2.0 and l_21_at_high_lambda > 2.0:
    print("Reference model incompatible - IRT formula doesn't fit data")
```

**Evidence in our experiments:**
- Œª=0.07: l_21 = 4.57
- Œª=0.15: l_21 = 4.06
- Conclusion: IRT incompatible regardless of Œª

**Action if incompatible:**
- Path A: Remove alignment losses, focus on performance
- Path B: Investigate reference model (validate M_ref, try different formula, use different theory)

#### 2. Does higher Œª improve or degrade alignment?

**Check:** Compare l_21, l_22, l_23 as Œª increases

**Diagnostic:**
```python
if l_21_increases_with_lambda or l_22_increases_with_lambda:
    print("Higher Œª makes alignment WORSE - model being forced into bad optimum")
else:
    print("Higher Œª helps alignment - just need more training/higher Œª")
```

**Evidence in our experiments:**
- Œª=0.07 ‚Üí Œª=0.15: l_21 improved (4.57 ‚Üí 4.06), l_22 degraded (0.033 ‚Üí 0.144)
- Conclusion: Mixed signal, but Œ≤ learning clearly harmed

**Action if degradation:**
- Higher Œª is counterproductive
- Choose Path A (remove alignment) or Path B (fix reference model first)

#### 3. Is scale collapse happening?

**Check:** Œ∏_std > 0.5 and Œ∏ range > 2.0

**Diagnostic:**
```python
if theta_std < 0.5:
    print("Scale collapse - model learning constant ability")
elif theta_std > 2.0:
    print("Healthy scale - individual differences learned")
```

**Evidence in our experiments:**
- Static IRT: Œ∏_std = 0.14 (collapsed)
- Dynamic IRT: Œ∏_std = 4.03 (healthy)
- Conclusion: Dynamic IRT solved this problem

**Action if collapsed:**
- Switch to dynamic IRT targets (time-varying Œ∏)
- Or add scale regularization pipeline

#### 4. Is Œ≤ learning working?

**Check:** l_22 < 0.10

**Diagnostic:**
```python
if l_22 > 0.10:
    print("Œ≤ embeddings not matching IRT difficulties")
    if l_22_worsens_with_higher_lambda:
        print("Higher Œª breaks Œ≤ learning - reduce Œª or increase c")
```

**Evidence in our experiments:**
- Œª=0.07: l_22 = 0.033 (excellent)
- Œª=0.15: l_22 = 0.144 (poor, 4.4√ó worse)
- Conclusion: Higher Œª breaks Œ≤ learning

#### 5. Is performance competitive?

**Check:** Compare AUC to baselines without IRT constraints

**Diagnostic:**
```python
if auc < baseline_without_irt:
    print("IRT constraints hurting performance")
    print("Choose: performance (Path A) or alignment (Path B)")
```

**Evidence in our experiments:**
- iKT3 (Œª=0.15): AUC = 0.7202
- simpleKT (no IRT): AUC = 0.7248
- Conclusion: IRT constraints slightly hurt performance

---

### Decision Tree

```
Start
  ‚îÇ
  ‚îú‚îÄ Is l_21 high (>2) across multiple Œª values?
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ YES ‚Üí Reference model incompatible
  ‚îÇ   ‚îÇ         ‚îú‚îÄ Choose Path A: Remove IRT alignment (Œª=0)
  ‚îÇ   ‚îÇ         ‚îî‚îÄ Choose Path B: Fix reference model first
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ NO ‚Üí Reference model compatible, continue
  ‚îÇ
  ‚îú‚îÄ Does higher Œª improve alignment?
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ YES ‚Üí Need more training with higher Œª
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ NO ‚Üí Higher Œª counterproductive
  ‚îÇ             ‚îî‚îÄ Choose Path A or B
  ‚îÇ
  ‚îú‚îÄ Is Œ∏_std < 0.5?
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ YES ‚Üí Scale collapse
  ‚îÇ   ‚îÇ         ‚îî‚îÄ Use dynamic IRT or scale pipeline
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ NO ‚Üí Scale healthy, continue
  ‚îÇ
  ‚îú‚îÄ Is l_22 > 0.10?
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ YES ‚Üí Œ≤ learning broken
  ‚îÇ   ‚îÇ         ‚îî‚îÄ Reduce Œª or increase c
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ NO ‚Üí Œ≤ learning working
  ‚îÇ
  ‚îî‚îÄ Is AUC competitive?
      ‚îÇ
      ‚îú‚îÄ YES ‚Üí Success, publish results
      ‚îÇ
      ‚îî‚îÄ NO ‚Üí Trade-off required
                ‚îî‚îÄ Choose Path A (performance) or Path B (alignment)
```

This protocol identifies **structural incompatibility** before attempting parameter tuning, avoiding wasted effort optimizing incompatible objectives.

## Critical Findings: Lambda Schedule and Performance Issues

### Experiment Results (Dec 7, 2025)

**Static IRT Baseline (Experiment 161656):**
- Test AUC: 0.7182, Accuracy: 0.7473
- **Scale collapse detected:** Œ∏_std = 0.14 (94% deflation vs target 2.5)
- Configuration: Œª_target=0.5, warmup_epochs=50, training_epochs=30
- Actual Œª at best epoch (7): 0.07

**Dynamic IRT (Experiment 686759):**
- Test AUC: 0.7202 (+0.29%), Accuracy: 0.7472 (equivalent)
- **Scale collapse SOLVED:** Œ∏_std = 4.03 (healthy, +2806% improvement)
- Configuration: Œª_target=0.5, warmup_epochs=50, training_epochs=30
- Actual Œª at best epoch (15): 0.15

**Key Discovery - Dynamic IRT Solves Scale Collapse:**
Dynamic time-varying Œ∏ trajectories naturally prevent scale collapse by forcing the model to learn individual differences across time, rather than optimizing for a constant value. This makes the scale regularization pipeline less critical.

### Performance Problem: Lambda Weight Too High

Despite solving scale collapse, **both experiments achieve below state-of-the-art performance:**
- iKT3 Dynamic: 0.7202 AUC
- simpleKT: 0.7248 AUC (baseline single-block)
- GainAKT2: 0.7224 AUC (tuned)
- AKT: ~0.80+ AUC (multi-block, complex)

**Root Cause Analysis:**

Test loss breakdown (Dynamic IRT, experiment 686759):
- l_bce: ~0.5 (reasonable)
- l_21 (performance alignment): 4.058 (high, but improved vs baseline 4.574)
- **l_22 (difficulty alignment): 0.144** (catastrophic, 338% worse than baseline 0.033)
- l_23 (ability alignment): 6.792 (30√ó higher than baseline - expected for dynamic trajectories)

**Critical Issue:** Even with adaptive Œª schedule, l_23 dominates the combined loss:
- At best epoch (15): Œª=0.15, so alignment weight = 0.15 √ó (4.058 + 0.144 + 6.792) = 1.66
- But l_23 alone contributes: 0.15 √ó 6.792 = 1.02 to total loss
- Meanwhile l_bce contributes: 0.85 √ó 0.5 = 0.425
- **Result:** Model optimizes Œ∏ matching at the expense of Œ≤ (difficulty) and predictions

**Lambda Schedule Implementation (Already Active):**
```python
Œª(t) = Œª_target √ó min(1, epoch / warmup_epochs)
```

Progression for current config (Œª_target=0.5, warmup=50, epochs=30):

| Epoch | Œª(t) | BCE Weight | IRT Weight | Status |
|-------|------|------------|------------|--------|
| 1     | 0.01 | 99%        | 1%         |        |
| 5     | 0.05 | 95%        | 5%         |        |
| 10    | 0.10 | 90%        | 10%        |        |
| 15    | 0.15 | 85%        | 15%        | ‚Üê Best |
| 30    | 0.30 | 70%        | 30%        | ‚Üê Final |

**The problem:** Even Œª=0.15 is too high because l_23 (dynamic trajectory matching) has huge magnitude (6.79) compared to l_bce (0.5), causing IRT alignment to dominate despite the 85%/15% weight split.

### Revised Approach

**Phase 1: Reduce Œª_target (Immediate Test)**
- Try Œª_target = 0.05 or 0.1 (currently 0.5)
- At epoch 15: Œª = 0.03 or 0.06 (vs current 0.15)
- Expected: Better Œ≤ learning (lower l_22), improved predictions
- Trade-off: Less Œ∏ alignment, but that's acceptable for performance

**Phase 2: Separate Lambda Components (Better Control)**
Instead of single Œª for all alignment losses:
```python
L = (1-Œª_pred) √ó l_bce + c √ó l_22 + Œª_21 √ó l_21 + Œª_23 √ó l_23
```

Recommended weights:
- Œª_21 = 0.1 (performance alignment - moderate)
- c = 0.01 (difficulty regularization - always active)
- Œª_23 = 0.01-0.02 (ability alignment - very low due to high magnitude)
- Result: Better control over each loss component

**Phase 3: Consider Removing l_23 Entirely (Radical)**
- Use only l_21 + c√ól_22 (no Œ∏ alignment)
- Rationale: If l_23 hurts more than it helps, why include it?
- Œ∏ values still computed for interpretability, just not aligned to IRT
- Trade-off: Less theoretically grounded, but potentially better performance

### Implementation Priority

1. **Immediate:** Test Œª_target = 0.05 with current setup
   - Quick experiment to validate hypothesis
   - Minimal code changes (update parameter_default.json)
   
2. **Short-term:** Implement separate lambda components
   - Better control over loss balance
   - Requires updating loss computation in irt_reference.py
   
3. **Optional:** Scale regularization pipeline (if needed)
   - May not be necessary with dynamic IRT
   - Keep documented as alternative for static targets

## Scale Regularization Pipeline

### Problem Statement

**Scale Collapse Issue (Solved by Dynamic IRT):** 

Previously with static IRT targets, the model learned near-constant Œ∏ values instead of learning the true distribution of student abilities. Evidence from experiments:
- Experiment 161656 (static): Œ∏_std = 0.14 vs target 2.5 (94% deflation)
- Œ∏_mean ‚âà -0.17 throughout training (essentially constant)
- Model outputs same Œ∏ for all students (no individual differences)
- Correlation = 0.07 (very poor alignment)

**Root Cause:** Model minimized l_23 = MSE(Œ∏_learned, Œ∏_IRT) by compressing Œ∏ values toward a constant (mean) instead of learning the correct scale and individual differences.

**Solution (Implemented):** Dynamic IRT targets with time-varying Œ∏ trajectories force the model to learn individual differences across time:
- Experiment 686759 (dynamic): Œ∏_std = 4.03 (healthy, +2806% improvement)
- Œ∏ range: -15.7 to +11.9 (29.6 units - strong individual differences)
- Natural prevention of scale collapse without explicit regularization

**Current Status:** Scale regularization pipeline documented below is **less critical** with dynamic IRT, but remains useful for:
1. Static IRT targets (if used)
2. Other reference models without dynamic variants
3. Additional regularization if needed

### Solution Requirements

The pipeline must satisfy four critical requirements:

1. **Prevent Scale Collapse:** Ensure Œ∏_std and Œ∏_mean match reference distribution
2. **Generalizable:** Work across different reference models (IRT, BKT) and datasets
3. **Preserve Interpretability:** Maintain absolute scale of Œ∏ values (not just relative ordering)
4. **Scale Matters:** Absolute Œ∏ values have theoretical meaning (e.g., Œ∏ = 0 is average ability)

### Multi-Stage Pipeline Architecture

The pipeline combines five complementary stages, each addressing a specific aspect of the scale collapse problem:

#### Stage 1: Z-Score Normalization (Correlation)

**Purpose:** Ensure learned parameters correlate with reference, regardless of scale

**Implementation:**
```python
def z_score_normalize(x, ref):
    """Normalize to match correlation, scale-invariant"""
    x_norm = (x - x.mean()) / (x.std() + 1e-8)
    ref_norm = (ref - ref.mean()) / (ref.std() + 1e-8)
    return F.mse_loss(x_norm, ref_norm)
```

**Properties:**
- ‚úÖ Generalizable: No dataset-specific parameters
- ‚úÖ Prevents gradient issues: Avoids large MSE when scales differ
- ‚ùå Loses absolute scale: Only preserves relative ordering
- **Weight:** Low (helps correlation, but not interpretability)

#### Stage 2: Scale Regularization (Interpretability)

**Purpose:** Match absolute scale (mean and std) of learned parameters

**Implementation:**
```python
def scale_regularization(theta_learned, theta_irt):
    """Match mean and standard deviation"""
    l_mean = F.mse_loss(theta_learned.mean(), theta_irt.mean())
    l_std = F.mse_loss(theta_learned.std(), theta_irt.std())
    return l_mean + l_std
```

**Properties:**
- ‚úÖ Preserves interpretability: Maintains absolute scale
- ‚úÖ Theory-grounded: Educational models require specific scales
- ‚ùå Dataset-specific: Requires target mean/std for each dataset
- **Weight:** High (critical for interpretability)

#### Stage 3: Collapse Prevention (Safety)

**Purpose:** Hard constraint preventing Œ∏_std from becoming too small

**Implementation:**
```python
def collapse_prevention(theta_learned, min_std=0.5):
    """Penalize if std falls below threshold"""
    current_std = theta_learned.std()
    if current_std < min_std:
        return (min_std - current_std) ** 2
    return torch.tensor(0.0, device=theta_learned.device)
```

**Properties:**
- ‚úÖ Generalizable: Simple threshold, no dataset-specific params
- ‚úÖ Fail-safe: Prevents catastrophic collapse
- ‚ùå Heuristic: Threshold choice may require tuning
- **Weight:** High when violated, zero otherwise

#### Stage 4: Distribution Shape Matching (Advanced)

**Purpose:** Match higher-order statistics (skewness, kurtosis)

**Implementation:**
```python
def distribution_shape_loss(theta_learned, theta_irt):
    """Match distribution shape beyond mean/std"""
    # Skewness: E[((x - Œº) / œÉ)¬≥]
    z_learned = (theta_learned - theta_learned.mean()) / theta_learned.std()
    z_irt = (theta_irt - theta_irt.mean()) / theta_irt.std()
    
    skew_learned = (z_learned ** 3).mean()
    skew_irt = (z_irt ** 3).mean()
    l_skew = (skew_learned - skew_irt) ** 2
    
    # Kurtosis: E[((x - Œº) / œÉ)‚Å¥]
    kurt_learned = (z_learned ** 4).mean()
    kurt_irt = (z_irt ** 4).mean()
    l_kurt = (kurt_learned - kurt_irt) ** 2
    
    return l_skew + l_kurt
```

**Properties:**
- ‚úÖ Fine-grained: Matches full distribution, not just moments
- ‚ùå Complex: May overfit to reference distribution
- ‚ùå Computationally expensive: Higher-order moments
- **Weight:** Low (optional refinement)

#### Stage 5: Ratio Constraints (Model-Specific)

**Purpose:** Leverage model-specific relationships (e.g., Œ∏/Œ≤ ratio for IRT)

**Implementation:**
```python
def irt_ratio_constraint(theta_learned, beta_learned, theta_irt, beta_irt):
    """For IRT: Ensure Œ∏ - Œ≤ distribution matches reference"""
    diff_learned = theta_learned.unsqueeze(1) - beta_learned  # [B, num_skills]
    diff_irt = theta_irt.unsqueeze(1) - beta_irt
    
    # Match mean and std of differences
    l_ratio_mean = F.mse_loss(diff_learned.mean(), diff_irt.mean())
    l_ratio_std = F.mse_loss(diff_learned.std(), diff_irt.std())
    
    return l_ratio_mean + l_ratio_std
```

**Properties:**
- ‚úÖ Theory-grounded: Uses model-specific structure (IRT formula)
- ‚ùå Not generalizable: Requires model-specific implementation
- ‚ùå Complex: Needs both Œ∏ and Œ≤ available
- **Weight:** Medium (useful for IRT, not applicable to other models)

### Combined Pipeline Loss

**Full formula:**
```python
l_23_pipeline = (
    w_zscore     √ó l_23_zscore +           # Stage 1: Correlation
    w_scale      √ó l_23_scale +            # Stage 2: Scale matching
    w_collapse   √ó l_23_collapse +         # Stage 3: Collapse prevention
    w_shape      √ó l_23_shape +            # Stage 4: Distribution shape
    w_ratio      √ó l_23_ratio              # Stage 5: Model-specific
)
```

**Recommended weights (for IRT):**
- `w_zscore = 0.1`: Low (helps but loses scale)
- `w_scale = 1.0`: High (critical for interpretability)
- `w_collapse = 10.0`: Very high when triggered (safety net)
- `w_shape = 0.05`: Very low (optional refinement)
- `w_ratio = 0.5`: Medium (IRT-specific)

**For generalizability across models (not just IRT):**
- Disable Stage 5 (ratio): Set `w_ratio = 0`
- Core pipeline: Stages 1-4 work for any reference model

### Implementation Plan

**Current Status:** Pipeline implementation is **DEFERRED** pending Phase 1 and Phase 2 experiments.

**Revised Priority:**

1. **Phase 1: Reduce Œª_target (IMMEDIATE - Dec 7, 2025)**
   - Test Œª_target = 0.05 (vs current 0.5)
   - Expected: Reduced IRT alignment weight, better Œ≤ learning, improved AUC
   - Files to modify: `configs/parameter_default.json`
   - Experiment duration: ~2 hours (30 epochs)

2. **Phase 2: Separate Lambda Components (SHORT-TERM)**
   - Implement individual lambda weights: Œª_21, c, Œª_23
   - Recommended: Œª_21=0.1, c=0.01, Œª_23=0.01-0.02
   - Files to modify: `pykt/reference_models/irt_reference.py`, `examples/train_ikt3.py`
   - Provides fine-grained control over loss balance

3. **Phase 3: Scale Pipeline (OPTIONAL - Only if needed)**
   - Dynamic IRT naturally prevents scale collapse
   - Pipeline may still be useful for static targets or other models
   - Implementation details below for future reference

**Decision Point:** If Phase 1 achieves competitive AUC (‚â•0.73), proceed with Phase 2 for fine-tuning. If not, reconsider architecture or investigate other issues beyond lambda weighting.

#### Pipeline Implementation (Optional - For Future Reference)

##### 1. Create Pipeline Function in `irt_reference.py`

Add to `IRTReferenceModel` class:

```python
def compute_l_23_with_pipeline(
    self,
    theta_learned: torch.Tensor,      # [B, L] or [B]
    theta_irt: torch.Tensor,          # [B, L] or [B]
    beta_learned: torch.Tensor = None, # [B, L] (optional, for ratio)
    beta_irt: torch.Tensor = None,     # [num_skills] (optional, for ratio)
    w_zscore: float = 0.1,
    w_scale: float = 1.0,
    w_collapse: float = 10.0,
    w_shape: float = 0.05,
    w_ratio: float = 0.5,
    min_std_threshold: float = 0.5
) -> Dict[str, torch.Tensor]:
    """
    Multi-stage pipeline for ability alignment with scale regularization.
    
    Prevents scale collapse while maintaining interpretability and generalizability.
    
    Returns:
        {
            'l_23_zscore': Stage 1 loss,
            'l_23_scale': Stage 2 loss,
            'l_23_collapse': Stage 3 loss,
            'l_23_shape': Stage 4 loss,
            'l_23_ratio': Stage 5 loss,
            'l_23_pipeline': Combined weighted loss
        }
    """
    losses = {}
    device = theta_learned.device
    
    # Stage 1: Z-score normalization (correlation)
    theta_l_norm = (theta_learned - theta_learned.mean()) / (theta_learned.std() + 1e-8)
    theta_i_norm = (theta_irt - theta_irt.mean()) / (theta_irt.std() + 1e-8)
    losses['l_23_zscore'] = F.mse_loss(theta_l_norm, theta_i_norm)
    
    # Stage 2: Scale regularization (interpretability)
    l_mean = F.mse_loss(theta_learned.mean(), theta_irt.mean())
    l_std = F.mse_loss(theta_learned.std(), theta_irt.std())
    losses['l_23_scale'] = l_mean + l_std
    
    # Stage 3: Collapse prevention (safety)
    current_std = theta_learned.std()
    if current_std < min_std_threshold:
        losses['l_23_collapse'] = (min_std_threshold - current_std) ** 2
    else:
        losses['l_23_collapse'] = torch.tensor(0.0, device=device)
    
    # Stage 4: Distribution shape matching (optional)
    z_l = (theta_learned - theta_learned.mean()) / (theta_learned.std() + 1e-8)
    z_i = (theta_irt - theta_irt.mean()) / (theta_irt.std() + 1e-8)
    skew_l = (z_l ** 3).mean()
    skew_i = (z_i ** 3).mean()
    kurt_l = (z_l ** 4).mean()
    kurt_i = (z_i ** 4).mean()
    losses['l_23_shape'] = (skew_l - skew_i) ** 2 + (kurt_l - kurt_i) ** 2
    
    # Stage 5: Ratio constraints (IRT-specific, optional)
    if beta_learned is not None and beta_irt is not None:
        # Expand theta to [B, num_skills] for broadcasting
        theta_l_exp = theta_learned.unsqueeze(1)  # [B, 1]
        theta_i_exp = theta_irt.unsqueeze(1)
        
        # Compute Œ∏ - Œ≤ differences
        diff_learned = theta_l_exp - beta_learned  # [B, L] - [B, L]
        diff_irt = theta_i_exp - beta_irt
        
        l_ratio_mean = F.mse_loss(diff_learned.mean(), diff_irt.mean())
        l_ratio_std = F.mse_loss(diff_learned.std(), diff_irt.std())
        losses['l_23_ratio'] = l_ratio_mean + l_ratio_std
    else:
        losses['l_23_ratio'] = torch.tensor(0.0, device=device)
    
    # Combined pipeline loss
    losses['l_23_pipeline'] = (
        w_zscore * losses['l_23_zscore'] +
        w_scale * losses['l_23_scale'] +
        w_collapse * losses['l_23_collapse'] +
        w_shape * losses['l_23_shape'] +
        w_ratio * losses['l_23_ratio']
    )
    
    return losses
```

#### 2. Add Config Parameters to `parameter_default.json`

Add new pipeline parameters to defaults section:

```json
{
  "defaults": {
    ...existing parameters...,
    "lambda_target": 0.5,
    "warmup_epochs": 50,
    "c_stability_reg": 0.01,
    
    "use_scale_pipeline": true,
    "w_zscore": 0.1,
    "w_scale": 1.0,
    "w_collapse": 10.0,
    "w_shape": 0.05,
    "w_ratio": 0.5,
    "min_std_threshold": 0.5
  },
  "types": {
    ...existing types...,
    "scale_pipeline": [
      "use_scale_pipeline",
      "w_zscore",
      "w_scale",
      "w_collapse",
      "w_shape",
      "w_ratio",
      "min_std_threshold"
    ]
  }
}
```

Update MD5 hash after changes:
```bash
python -c "
import json, hashlib
data = json.load(open('configs/parameter_default.json'))
md5 = hashlib.md5(json.dumps(data['defaults'], sort_keys=True).encode()).hexdigest()
data['md5'] = md5
json.dump(data, open('configs/parameter_default.json', 'w'), indent=2)
print(f'Updated MD5: {md5}')
"
```

#### 3. Update Training to Use Pipeline Losses

Modify `compute_alignment_losses()` in `irt_reference.py`:

```python
def compute_alignment_losses(
    self,
    model_outputs: Dict[str, torch.Tensor],
    targets: Dict[str, torch.Tensor],
    lambda_weights: Dict[str, float]
) -> Dict[str, torch.Tensor]:
    """Compute IRT alignment losses with optional scale pipeline."""
    device = next(iter(model_outputs.values())).device
    
    # ... l_21 and l_22 computation (unchanged) ...
    
    # l_23: Ability alignment with scale pipeline
    if 'theta_t_learned' in model_outputs and 'theta_irt' in targets:
        theta_t_learned = model_outputs['theta_t_learned']  # [B, L]
        theta_irt = targets['theta_irt']
        
        # Use pipeline if enabled
        use_pipeline = lambda_weights.get('use_scale_pipeline', False)
        
        if use_pipeline:
            # Multi-stage pipeline
            pipeline_losses = self.compute_l_23_with_pipeline(
                theta_learned=theta_t_learned,
                theta_irt=theta_irt,
                beta_learned=model_outputs.get('beta_learned'),
                beta_irt=targets.get('beta_irt'),
                w_zscore=lambda_weights.get('w_zscore', 0.1),
                w_scale=lambda_weights.get('w_scale', 1.0),
                w_collapse=lambda_weights.get('w_collapse', 10.0),
                w_shape=lambda_weights.get('w_shape', 0.05),
                w_ratio=lambda_weights.get('w_ratio', 0.5),
                min_std_threshold=lambda_weights.get('min_std_threshold', 0.5)
            )
            
            # Use pipeline loss as l_23
            l_23 = pipeline_losses['l_23_pipeline']
            
            # Return all stage losses for logging
            return {
                'l_21_performance': l_21,
                'l_22_difficulty': l_22,
                'l_23_ability': l_23,
                'l_23_zscore': pipeline_losses['l_23_zscore'],
                'l_23_scale': pipeline_losses['l_23_scale'],
                'l_23_collapse': pipeline_losses['l_23_collapse'],
                'l_23_shape': pipeline_losses['l_23_shape'],
                'l_23_ratio': pipeline_losses['l_23_ratio'],
                'l_align_total': l_21 + l_23
            }
        else:
            # Original direct MSE (baseline)
            l_23 = F.mse_loss(theta_t_learned, theta_irt, reduction='mean')
    else:
        l_23 = torch.tensor(0.0, device=device)
        print("‚ö†Ô∏è  Warning: Cannot compute l_23 - missing theta_t_learned or theta_irt")
    
    return {
        'l_21_performance': l_21,
        'l_22_difficulty': l_22,
        'l_23_ability': l_23,
        'l_align_total': l_21 + l_23
    }
```

#### 4. Log All Stage Losses Separately in Metrics

Update `train_epoch()` in `train_ikt3.py` to log pipeline stages:

```python
# In train_epoch() function, after computing losses:

# Log standard losses
batch_metrics = {
    'loss': loss.item(),
    'l_bce': l_bce.item(),
    'l_21': alignment_losses.get('l_21_performance', torch.tensor(0.0)).item(),
    'l_22': alignment_losses.get('l_22_difficulty', torch.tensor(0.0)).item(),
    'l_23': alignment_losses.get('l_23_ability', torch.tensor(0.0)).item(),
}

# Log pipeline stage losses if using pipeline
if args.use_scale_pipeline:
    batch_metrics.update({
        'l_23_zscore': alignment_losses.get('l_23_zscore', torch.tensor(0.0)).item(),
        'l_23_scale': alignment_losses.get('l_23_scale', torch.tensor(0.0)).item(),
        'l_23_collapse': alignment_losses.get('l_23_collapse', torch.tensor(0.0)).item(),
        'l_23_shape': alignment_losses.get('l_23_shape', torch.tensor(0.0)).item(),
        'l_23_ratio': alignment_losses.get('l_23_ratio', torch.tensor(0.0)).item(),
    })

# ... rest of metrics ...
```

Update `metrics_epoch.csv` headers to include pipeline losses:
- Standard: epoch, loss, l_bce, l_21, l_22, l_23, val_auc, val_acc
- With pipeline: + l_23_zscore, l_23_scale, l_23_collapse, l_23_shape, l_23_ratio

#### 5. Document Each Stage's Purpose

Add to experiment README and metrics documentation:

**Pipeline Stage Descriptions:**

| Stage | Loss Name | Purpose | When Active | Expected Value |
|-------|-----------|---------|-------------|----------------|
| **1. Z-Score** | `l_23_zscore` | Correlation alignment (scale-free) | Always | < 1.0 (normalized MSE) |
| **2. Scale** | `l_23_scale` | Mean/std matching (interpretability) | Always | < 5.0 (depends on scale) |
| **3. Collapse** | `l_23_collapse` | Hard constraint: Œ∏_std > threshold | Only if Œ∏_std < 0.5 | 0.0 (if std healthy) |
| **4. Shape** | `l_23_shape` | Skewness/kurtosis matching | Always (if w_shape > 0) | < 1.0 (normalized) |
| **5. Ratio** | `l_23_ratio` | Œ∏-Œ≤ difference distribution (IRT) | Only for IRT with Œ≤ | < 10.0 (depends on scale) |

**Interpretation Guide:**

- **Healthy training:** l_23_collapse = 0 throughout (std > threshold)
- **Scale collapse detected:** l_23_collapse > 0 and increasing
- **Good correlation:** l_23_zscore < 0.5
- **Good interpretability:** l_23_scale < 2.0 and Œ∏_std ‚âà target_std
- **Pipeline working:** l_23_pipeline < l_23_direct (baseline MSE)

### Expected Outcomes

**With pipeline enabled (`use_scale_pipeline=true`):**

| Metric | Baseline (No Pipeline) | With Pipeline | Improvement |
|--------|------------------------|---------------|-------------|
| Œ∏_std | 0.14 (collapsed) | 2.3-2.7 (healthy) | 16-19√ó larger |
| Œ∏_mean | -0.17 (wrong) | -0.05 to 0.05 (correct) | Matches target |
| Correlation | 0.07 (poor) | 0.65-0.85 (good) | 9-12√ó better |
| l_23 | 4.5 (high MSE) | 0.5-1.5 (low) | 3-9√ó smaller |
| Interpretability | ‚ùå No (constant Œ∏) | ‚úÖ Yes (varied Œ∏) | Fixed |

**Generalizability:**

- ‚úÖ Works with IRT (tested)
- ‚úÖ Works with BKT (Stages 1-4, disable Stage 5)
- ‚úÖ Works with other datasets (no hardcoded thresholds except min_std)
- ‚úÖ Maintains interpretability (absolute scale preserved)

### Validation Protocol

After implementing pipeline, validate with these checks:

1. **Scale Health Check:**
   ```python
   assert val_theta_std > 0.5, "Scale collapse detected!"
   assert 0.8 * target_std < val_theta_std < 1.2 * target_std, "Scale mismatch!"
   ```

2. **Correlation Check:**
   ```python
   assert val_correlation > 0.5, "Poor alignment!"
   ```

3. **Individual Differences Check:**
   ```python
   theta_range = val_theta_max - val_theta_min
   assert theta_range > 2.0, "No individual differences learned!"
   ```

4. **Pipeline Stage Check:**
   ```python
   assert l_23_collapse < 0.01, "Collapse prevention triggered!"
   assert l_23_scale < l_23_zscore, "Scale regularization not helping!"
   ```

### Ablation Studies

Test each stage's contribution:

| Experiment | Enabled Stages | Purpose |
|------------|----------------|---------|
| Baseline | None (direct MSE) | Reference |
| Ablation 1 | Stage 1 only (z-score) | Test correlation alone |
| Ablation 2 | Stage 2 only (scale) | Test scale matching alone |
| Ablation 3 | Stages 1+2 | Test synergy |
| Ablation 4 | Stages 1+2+3 | Test collapse prevention |
| Full Pipeline | Stages 1+2+3+4+5 | All features |

Expected finding: Stages 1+2 provide 80% of benefit, Stage 3 is safety net, Stages 4+5 provide marginal refinement.

 