## Abstract

While deep knowledge tracing (DKT) models provide state-of-the-art predictive accuracy, their black-box architectures often limit their utility in real-world educational settings where actionable pedagogical insights are required. We present iDKT, a novel Transformer-based framework that bridges this gap by providing the predictive power of deep learning while achieving intrinsic interpretability. iDKT accomplishes this through *structural grounding*, an architectural principle that anchors deep latent representations to educational constructs defined by established pedagogical models.

We validated iDKT using Bayesian Knowledge Tracing (BKT), demonstrating that it maintains DKT-level predictive performance while providing more granular, student-specific insights than standard theoretical baselines. Specifically, the model transforms population-level priors into individualized distributions, identifying initial knowledge gaps and detecting diverse learning velocities. This enables educators to move beyond simple performance prediction to implement precise diagnostic placement and dynamic pacing in diverse learning environments. By anchoring internal latent representations to the conceptual space of pedagogical theory, iDKT offers both a robust methodology for evaluating model interpretability and a practical tool for researchers and practitioners to improve educational effectiveness through theory-informed, data-driven personalization.

